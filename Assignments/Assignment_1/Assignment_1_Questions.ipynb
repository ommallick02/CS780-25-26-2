{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS780: Deep Reinforcement Learning Assignment - 1\n",
        "***Instructor: Ashutosh Modi***\n",
        "\n",
        "\n",
        "- Submission Deadline: 07/02/2026; 11:59 PM\n",
        "- Submission Form: [https://forms.gle/KZ8EZwUuddw1Gh648](https://forms.gle/KZ8EZwUuddw1Gh648)\n"
      ],
      "metadata": {
        "id": "ZdY-1xnC_4O5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instructions\n",
        "Read all the instructions below carefully before you start working on the assignment.\n",
        "- The purpose of this course is that you learn RL and the best way to do that is by implementation and experimentation.\n",
        "- The assignment requires your to implement some algorithms and you are required report your findings after experimenting with those algorithms.\n",
        "- You are to treat this colab notebook as a mix of report and code repo, such that all the code blocks are executable from end-to-end and the write about your observations and findings in relecant markdown cells.\n",
        "- You are expected to implement algorithms on your own and not copy it from other sources/class mates.\n",
        "Of course, you can refer to lecture slides.\n",
        "- If you use any reference or material (including code), please cite the source, else it will be considered\n",
        "plagiarism. But referring to other sources that directly solve the problems given in the assignment is not\n",
        "allowed. There is a limit to which you can refer to outside material.\n",
        "- This is an individual assignment.\n",
        "- In case your solution is found to have an overlap with solution by someone else (including external sources),\n",
        "all the parties involved will get zero in this and all future assignments plus further more penalties in the\n",
        "overall grade. We will check not just for lexical but also semantic overlap. Same applies for the code as\n",
        "well. **Even an iota of cheating would NOT be tolerated**. If you cheat one line or cheat one page\n",
        "the penalty would be same.\n",
        "- Be a smart agent, think long term, if you cheat we will discover it somehow, the price you would be paying\n",
        "is not worth it.\n",
        "- In case you are struggling with the assignment, seek help from TAs. Cheating is not an option! I respect\n",
        "honesty and would be lenient if you are not able to solve some questions due to difficulty in understanding.\n",
        "Remember we are there to help you out, seek help if something is difficult to understand.\n",
        "- The deadline for the submission is given above. Submit at least 30 minutes before the deadline, lot can\n",
        "happen at the last moment, your internet can fail, there can be a power failure, you can be abducted by\n",
        "aliens, etc.\n",
        "- You have to submit your assignment via following Google Form (link above). You will be sharing your\n",
        "code and a report having the details about the code and answer to questions in the assignment.\n",
        "- The form would close after the deadline and we will not accept any solution. No reason what-so-ever\n",
        "would be accepted for not being able to submit before the deadline.\n",
        "- Since the assignment involves experimentation, reporting your results and observations, there is a lot\n",
        "of scope for creativity and innovation and presenting new perspectives. Such efforts would be highly\n",
        "appreciated and accordingly well rewarded. Be an exploratory agent!\n",
        "In your plots, have a clear legend and clear lines, etc. Of course you would generating the plots in your\n",
        "code but you must also put these plots in your report. Generate high resolution pdf/svg version of the\n",
        "plots so that it doesnâ€™t pixilate on zooming.\n",
        "- For implementing a new environment in Gymaniusm, we have already shared a document about the\n",
        "process, please refer to it. You are not required to render the environment on the screen/terminal. **Do\n",
        "remember to set the seed, this will be useful for reproducing your experiments. And we\n",
        "will use the seed provided by you to verify your results. Also for each instance of the\n",
        "environment that you create use a different seed and save these seeds, these will be useful\n",
        "for reproducing the results**. Do not set the seed to 42 :P\n",
        "- For all experiments, report about the seed used in the code documentation and also in your report, write\n",
        "about the seed used.\n",
        "- In your report write about all things that are not obvious from the code e.g., if you have made any\n",
        "assumptions, references/sources, running time, etc.\n",
        "<!-- - Code should be written in Jupyter notebook and keep it very well documented (there are separate marks\n",
        "for documentation). Include readme file that tells about how to run the code. Zip your documented\n",
        "Jupyter notebook and readme. -->"
      ],
      "metadata": {
        "id": "Zm0LiG-jAgM5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1: Multi-armed Bandits\n",
        "$(10+10+60+20+20+20+20+20+20+20=220\\text{ points})$\n",
        "\n",
        "In lecture 6 we learnt about Multi-armed bandit. In this problem, you will be implementing Multi-armed bandit with different strategies. The aim is to compare the performance of different strategies.\n",
        "In particular, you will be implementing 2-armed Bernoulli Bandit (Figure (a)) and 10-armed Gaussian Bandit (Figure (b)).\n",
        "\n",
        "![multi-armed bandits](https://i.postimg.cc/d0LBhpRH/Screenshot-2026-01-18-at-2-55-07-PM.png)\n",
        "\n",
        "\n",
        "**2-armed Bernoulli Bandit:** The mathematical formulation is as follows:\n",
        "\n",
        "$$\n",
        "R_0 \\sim \\text{Bernoulli}(\\alpha)\n",
        "$$\n",
        "$$\n",
        "R_1 \\sim \\text{Bernoulli}(\\beta)\n",
        "$$\n",
        "\n",
        "**10-armed Gaussian Bandit:** The mathematical formulation is as follows:\n",
        "\n",
        "$$\n",
        "q_*(k) \\sim N(\\mu=0, \\sigma^2=1)\n",
        "$$\n",
        "$$\n",
        "R_k \\sim N(\\mu=q_*(k), \\sigma^2=1)\n",
        "$$\n",
        "\n",
        "Note that in 10-armed Gaussian Bandit, for each arm $k$, the action value $q_*(k)$ is sampled from a standard Gaussian distribution and reward for the corresponding arm is sample from a gaussian distribution which has mean $q_*(k)$ and unit standard deviation.\n",
        "\n"
      ],
      "metadata": {
        "id": "WJC77IihV08W"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wpY0SlOceNhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install gymnasium\n",
        "!pip install gymnasium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROcZiqyVeLtF",
        "outputId": "a08084da-5502-4a1d-cdd2-c22d89cc80b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.3)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "# For reproducibility\n",
        "np.random.seed(40)"
      ],
      "metadata": {
        "id": "Jt78bW5OeOJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Create 2-armed bandit environment (10 points)\n",
        "In Gymnasium create the environment for 2-armed Bernoulli Bandit. The environment should take $\\alpha$ and $\\beta$ as input parameters and simulate 2-armed bandit accordingly. Once you have implemented the environment, run it using different values of $\\alpha$ and $\\beta$ to make sure it is executing as expected. For, example, you can try with $(\\alpha, \\beta) = (0, 0), (1, 0), (0, 1), (1, 1), (0.5, 0.5)$, etc. Report about your test cases and how they point towards the correct implementation. Also report about your general observations."
      ],
      "metadata": {
        "id": "_2dd3x5FXWT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoArmedBernoulliBandit(gym.Env):\n",
        "    \"\"\"\n",
        "    Gymnasium environment for a 2-armed Bernoulli Bandit.\n",
        "    Action 0 - Bernoulli(alpha)\n",
        "    Action 1 - Bernoulli(beta)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, alpha, beta):\n",
        "        \"\"\"\n",
        "        Initialize the environment.\n",
        "\n",
        "        Args:\n",
        "            alpha (float): Success probability for Arm 0.\n",
        "            beta (float): Success probability for Arm 1.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Write your code here\n",
        "        pass\n",
        "\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"\n",
        "        Reset the environment.\n",
        "        \"\"\"\n",
        "        super().reset(seed=seed)\n",
        "        return 0, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Execute one time step within the environment.\n",
        "\n",
        "        Args:\n",
        "            action (int): The selected arm (0 or 1).\n",
        "\n",
        "        Returns: tuple of (observation, reward, terminated, truncated, info)\n",
        "            observation (int): 0 (dummy state).\n",
        "            reward (float): 1.0 (success) or 0.0 (failure).\n",
        "            terminated (bool): False (episodic tasks usually end, but bandits are often treated as infinite or fixed step).\n",
        "            truncated (bool): False.\n",
        "            info (dict): Empty dictionary.\n",
        "        \"\"\"\n",
        "        # Write your code here\n",
        "        pass"
      ],
      "metadata": {
        "id": "BKrHDfuOGS0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_bernoulli_bandit(alpha, beta):\n",
        "    \"\"\"\n",
        "    Test the 2-armed Bernoulli Bandit environment to verify probabilities.\n",
        "\n",
        "    Args:\n",
        "        alpha (float): Success probability for Arm 0.\n",
        "        beta (float): Success probability for Arm 1.\n",
        "    \"\"\"\n",
        "    steps = 10_000\n",
        "    env = TwoArmedBernoulliBandit(___, ____)\n",
        "    env._____()\n",
        "    # Write your code here\n",
        "\n",
        "    print(f\"(alpha, beta) = ({alpha}, {beta})\")\n",
        "    print(f\"  Arm 0 mean reward = {_____}\")\n",
        "    print(f\"  Arm 1 mean reward = {_____}\")\n",
        "    print(\"-\" * 40)\n"
      ],
      "metadata": {
        "id": "Wwb2l4-OHwZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "try for different values of alpha and beta!\n",
        "test_cases = [\n",
        "    (0, 0),\n",
        "    (1, 0),\n",
        "    (0, 1),\n",
        "    (1, 1),\n",
        "    (0.5, 0.5),\n",
        "    (0.25, 0.75),\n",
        "    (0.75, 0.25),\n",
        "    (0.1, 0.9),\n",
        "    (0.9, 0.1),\n",
        "]\n",
        "\n",
        "for alpha, beta in test_cases:\n",
        "    test_bernoulli_bandit(alpha, beta)\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "ucGVntCFIFdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "[Write your observations about the question and its solution here]"
      ],
      "metadata": {
        "id": "XbkdeZsMfljE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Create 10-armed bandit environment (10 points)\n",
        "Similarly, in Gymnasium create the environment for 10-armed Gaussian Bandit. Make sure it is executing as expected by creating certain test cases, e.g., by playing with $\\sigma$. Report about your test cases and how they point towards the correct implementation. Also report about your general observations.\n",
        "\n"
      ],
      "metadata": {
        "id": "BP442yJQX4aE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TenArmedGaussianBandit(gym.Env):\n",
        "    \"\"\"\n",
        "    Gymnasium environment for the 10-armed Gaussian Bandit.\n",
        "\n",
        "    Mathematical Formulation:\n",
        "        q*(k) ~ N(0, 1)\n",
        "        R_t ~ N(q*(A_t), sigma^2)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_arms=10, sigma=1.0):\n",
        "        \"\"\"\n",
        "        Initialize the environment.\n",
        "\n",
        "        Args:\n",
        "            num_arms (int): Number of arms (default = 10)\n",
        "            sigma (float): Standard deviation of reward noise\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Write your code here\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"\n",
        "        Reset the environment.\n",
        "\n",
        "        Returns:\n",
        "            observation (int): Dummy observation\n",
        "            info (dict)\n",
        "        \"\"\"\n",
        "        # Write your code here\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Execute one step of the environment.\n",
        "\n",
        "        Args:\n",
        "            action (int): Selected arm (0 to num_arms - 1)\n",
        "\n",
        "        Returns: tuple of (observation, reward, terminated, truncated, info)\n",
        "            observation (int): 0 (dummy state).\n",
        "            reward (float): Sampled Gaussian reward\n",
        "            terminated (bool): False (episodic tasks usually end, but bandits are often treated as infinite or fixed step).\n",
        "            truncated (bool): False.\n",
        "            info (dict): Empty dictionary.\n",
        "        \"\"\"\n",
        "        # Write your code here\n",
        "\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "-w8Au46dIhq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_gaussian_bandit(sigma, arm=0):\n",
        "    \"\"\"\n",
        "    Pull a single arm repeatedly to verify Gaussian reward statistics.\n",
        "\n",
        "    Verify two things:\n",
        "      1. Does the sample mean converge to q*(k)?\n",
        "      2. Does the sample standard deviation match sigma?\n",
        "\n",
        "    Args:\n",
        "        sigma (float): Standard deviation of Gaussian reward noise\n",
        "        arm (int): Arm index to test (0 to 9)\n",
        "    \"\"\"\n",
        "    steps = 10_000\n",
        "    # Write your code here\n",
        "    print(\"-\" * 45)\n"
      ],
      "metadata": {
        "id": "vUCw2pgcKR6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Test with different noise levels\n",
        "sigmas = [0.1, 0.5, 1.0, 2.0, 5.0]\n",
        "\n",
        "for sigma in sigmas:\n",
        "    test_gaussian_bandit(sigma=sigma)\n",
        "'''"
      ],
      "metadata": {
        "id": "ANfcrvD7Klw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "[Write your observations about the question and its solution here]"
      ],
      "metadata": {
        "id": "VD5pC--qfj7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Agents ($10 \\times 6 = 60$ points)"
      ],
      "metadata": {
        "id": "UdA0-gXdHTbT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. (a) Greedy agent (10 points)\n",
        "\n",
        "Create a function that implements the **Pure Exploitation (Greedy) strategy** (the function should have same signature as shown in the lecture). Run this for 2-armed Bernoulli Bandit to generate a table of actions and rewards (as in the lecture) and manually verify that the strategy is working as expected.\n"
      ],
      "metadata": {
        "id": "-yJCma1LYs-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pure_exploitation(env, max_steps):\n",
        "    \"\"\"\n",
        "    Implement the Pure Exploitation (Greedy) strategy for a bandit problem.\n",
        "\n",
        "    Args:\n",
        "        env (gym.Env): Bandit environment with discrete action space.\n",
        "        max_steps (int): Number of interaction steps.\n",
        "\n",
        "    Returns:\n",
        "        actions (np.ndarray): Actions selected at each step.\n",
        "        rewards (np.ndarray): Rewards received at each step.\n",
        "    \"\"\"\n",
        "    # Write your code here\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "Y4pp7zG1K9Hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_interaction_table(actions, rewards, rows=15):\n",
        "    \"\"\"\n",
        "    Prints a table of actions and rewards\n",
        "    \"\"\"\n",
        "    print(\"Step | Action | Reward\")\n",
        "    print(\"----------------------\")\n",
        "    for i in range(min(rows, len(actions))):\n",
        "        print(f\"{i:>4} | {actions[i]:>6} | {rewards[i]:>6}\")"
      ],
      "metadata": {
        "id": "jlHEH5BuMYiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Run it for 2-armed Bernoulli Bandit to generate a table of actions and rewards (use print_interaction_table) and\n",
        "manually verify that the strategy is working as expected.\n",
        "'''\n",
        "\n",
        "alpha, beta = ____, ____\n",
        "env = TwoArmedBernoulliBandit(____, ____)\n",
        "\n",
        "np.random.seed(12345)\n",
        "env.reset(seed=12345)\n",
        "\n",
        "actions, rewards = ______________\n",
        "print_interaction_table(____, ____)"
      ],
      "metadata": {
        "id": "CRkt-JHtMnz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "[Write your observations about the question and its solution here]"
      ],
      "metadata": {
        "id": "hppl47Mvfivo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. (b) Exploration Agent (10 points)\n",
        "\n",
        "\n",
        "Create a function that implements the **Pure Exploration strategy** (the function should have same signature as shown in the lecture). Run this for 2-armed Bernoulli Bandit to generate a table of actions and rewards (as in the lecture) and manually verify that the strategy is working as expected.\n",
        "\n"
      ],
      "metadata": {
        "id": "WCNLd7JkYjsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pure_exploration(env, max_steps):\n",
        "    \"\"\"\n",
        "    Implements the Pure Exploration strategy for a bandit problem.\n",
        "\n",
        "    Args:\n",
        "        env (gym.Env): Bandit environment with discrete action space.\n",
        "        max_steps (int): Number of interaction steps.\n",
        "\n",
        "    Returns:\n",
        "        actions (np.ndarray): Actions selected at each step.\n",
        "        rewards (np.ndarray): Rewards received at each step.\n",
        "    \"\"\"\n",
        "    # Write your code here\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "Ebw_9ENiPfWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Run it for 2-armed Bernoulli Bandit to generate a table of actions and rewards (use print_interaction_table)  and\n",
        "manually verify that the strategy is working as expected.\n",
        "'''\n",
        "\n",
        "alpha, beta = ____, _____\n",
        "env = TwoArmedBernoulliBandit(____, ____)\n",
        "\n",
        "np.random.seed(12345)\n",
        "env.reset(seed=12345)\n",
        "\n",
        "actions, rewards = _______\n",
        "print_interaction_table(____, _____)"
      ],
      "metadata": {
        "id": "Jx-VtiVMNA9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "[Write your observations about the question and its solution here]"
      ],
      "metadata": {
        "id": "8RPDi-4Yfhrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. (c) $\\epsilon $-greedy agent (10 points)\n",
        "\n",
        "Create a function that implements the **epsilon-Greedy strategy** (the function should have same signature as shown in the lecture). Run this for 2-armed Bernoulli Bandit with different values of epsilon, ranging from small to large values. Verify that your implementation is working.\n"
      ],
      "metadata": {
        "id": "Xq6WBQwZZTLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_greedy(env, max_steps, epsilon):\n",
        "    \"\"\"\n",
        "    Implement the Epsilon-Greedy strategy for a bandit problem.\n",
        "\n",
        "    Args:\n",
        "        env (gym.Env): Bandit environment with discrete action space.\n",
        "        max_steps (int): Number of interaction steps.\n",
        "        epsilon (float): Probability of choosing a random action (exploration).\n",
        "\n",
        "    Returns:\n",
        "        actions (np.ndarray): Actions selected at each step.\n",
        "        rewards (np.ndarray): Rewards received at each step.\n",
        "    \"\"\"\n",
        "    # Write your code here\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "iSER9V3tZaQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Run it for 2-armed Bernoulli Bandit to generate a table of actions and rewards (use print_interaction_table)  and\n",
        "manually verify that the strategy is working as expected.\n",
        "\n",
        "use diff values of epsilons!\n",
        "epsilons = [0.01, 0.1, 0.5, 1.0]\n",
        "'''\n",
        "\n",
        "alpha, beta = ______, ______\n",
        "env = TwoArmedBernoulliBandit(____, ____)\n",
        "\n",
        "# Write your code here"
      ],
      "metadata": {
        "id": "Vl0eEphhNTxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "[Write your observations about the question and its solution here]"
      ],
      "metadata": {
        "id": "q5ldLu_NfgsB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. (d) Decaying $\\epsilon $-greedy agent (10 points)\n",
        "\n",
        "\n",
        "Create a function that implements the **decaying epsilon-Greedy strategy** (the function should have same signature as shown in the lecture). You can try two different versions of decay: linear and exponential. Start with the value of 1.0 for epsilon and decay it linearly/exponentially with pre-defined rate to 0.0. You can try with different rates of decays. The type of decay and the final decay rate are input parameters to the function, include these in the function definition.\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "rAG2WYCkZbJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decaying_epsilon_greedy(env, max_steps,\n",
        "                            epsilon_start,\n",
        "                            epsilon_end,\n",
        "                            decay_type):\n",
        "    \"\"\"\n",
        "    Implements a decaying Epsilon-Greedy strategy.\n",
        "\n",
        "    Args:\n",
        "        env (gym.Env): Bandit environment with discrete action space.\n",
        "        max_steps (int): Number of interaction steps.\n",
        "        epsilon_start (float): Initial value of epsilon.\n",
        "        epsilon_end (float): Final value of epsilon.\n",
        "        decay_type (str): Type of decay ('linear' or 'exponential').\n",
        "\n",
        "    Returns:\n",
        "        actions (np.ndarray): Actions selected at each step.\n",
        "        rewards (np.ndarray): Rewards received at each step.\n",
        "    \"\"\"\n",
        "    # Write your code here\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "ViVvJ1YoWchC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Run it for 2-armed Bernoulli Bandit to generate a table of actions and rewards (use print_interaction_table)  and\n",
        "manually verify that the strategy is working as expected.\n",
        "\n",
        "play with diff start and end values!\n",
        "configs = [\n",
        "    (\"linear\", 1.0, 0.0),\n",
        "    (\"exponential\", 1.0, 0.01)\n",
        "]\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "alpha, beta = ____, _____\n",
        "env = TwoArmedBernoulliBandit(____, _____)\n",
        "\n",
        "configs = [\n",
        "    ______________\n",
        "]\n",
        "# Write your code here\n"
      ],
      "metadata": {
        "id": "G0bHaPmXNhLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "[Write your observations about the question and its solution here]"
      ],
      "metadata": {
        "id": "SYll8uhlfflr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. (e) Softmax agent (10 points)\n",
        "\n",
        "\n",
        "Create a function that implements the **Softmax strategy** (the function should have same signature as shown in the lecture). You would have to play with the initial temperature parameter. For example, you start with initial temperature of 100 and decay it linearly to 0.01 or you start with initial temperature of infty and decay it linearly to 0.005, etc.\n"
      ],
      "metadata": {
        "id": "UC_1eYZcZh19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(env, max_steps,\n",
        "                     temp_start,\n",
        "                     temp_end,\n",
        "                     decay_type):\n",
        "    \"\"\"\n",
        "    Implements the Softmax (Boltzmann) exploration strategy.\n",
        "\n",
        "    Args:\n",
        "        env (gym.Env): Bandit environment with discrete action space.\n",
        "        max_steps (int): Number of interaction steps.\n",
        "        temp_start (float): Initial temperature (Tau).\n",
        "        temp_end (float): Final temperature.\n",
        "        decay_type (str): 'linear' or 'exponential'\n",
        "\n",
        "    Returns:\n",
        "        actions (np.ndarray): Actions selected at each step.\n",
        "        rewards (np.ndarray): Rewards received at each step.\n",
        "    \"\"\"\n",
        "    # Write your code here\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "S36OkoO_ZoUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Run it for 2-armed Bernoulli Bandit to generate a table of actions and rewards (use print_interaction_table)  and\n",
        "manually verify that the strategy is working as expected.\n",
        "\n",
        "play with diff start and end values!\n",
        "configs = [\n",
        "    (\"linear\", 100.0, 0.01),\n",
        "    (\"linear\", 1e6, 0.005),\n",
        "    (\"exponential\", 100.0, 0.01)\n",
        "]\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "alpha, beta = _____, ______\n",
        "env = TwoArmedBernoulliBandit(____, _____)\n",
        "\n",
        "configs = [\n",
        "    ___________________\n",
        "]\n",
        "# Write your code here\n"
      ],
      "metadata": {
        "id": "c1QrCj0MN8E3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "[Write your observations about the question and its solution here]"
      ],
      "metadata": {
        "id": "dYPDVgy0feRD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. (f) UCB agent (10 points)\n",
        "\n",
        "\n",
        "\n",
        "Create a function that implements the **UCB strategy** (the function should have same signature as shown in the lecture). You would have to play with the `c` parameter. For example, you can try `c=0.2`, `c=0.5`, etc.\n"
      ],
      "metadata": {
        "id": "sMIX5bn4ZqZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ucb_strategy(env, max_steps, c):\n",
        "    \"\"\"\n",
        "    Implement the Upper Confidence Bound (UCB) strategy.\n",
        "\n",
        "    Args:\n",
        "        env (gym.Env): Bandit environment with discrete action space.\n",
        "        max_steps (int): Number of interaction steps.\n",
        "        c (float): Exploration coefficient controlling optimism.\n",
        "\n",
        "    Returns:\n",
        "        actions (np.ndarray): Actions selected at each step.\n",
        "        rewards (np.ndarray): Rewards received at each step.\n",
        "    \"\"\"\n",
        "    # Write your code here\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "2BUgpXRTgB7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Run it for 2-armed Bernoulli Bandit to generate a table of actions and rewards (use print_interaction_table)  and\n",
        "manually verify that the strategy is working as expected.\n",
        "\n",
        "play with diff c values\n",
        "c_values = [0.0, 0.2, 0.5, 1.0]\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "alpha, beta = ______, ______\n",
        "env = TwoArmedBernoulliBandit(______, ______)\n",
        "\n",
        "c_values = _______\n",
        "\n",
        "# Write your code here"
      ],
      "metadata": {
        "id": "yytSssWAOIdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "[Write your observations about the question and its solution here]"
      ],
      "metadata": {
        "id": "4QmERF2bfcNC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. 2-armed bandit testbed (20 points)\n",
        "\n",
        "Create 50 different 2-armed Bernoulli Bandit environments by sampling different values of $\\alpha$ and $\\beta$ from a standard uniform distribution ($U(0,1)$). Run each of the agents above (6 in total) for 1000 time-steps (this is one run) for each instance of the environment. At each time step record the received reward. For a given agent, at each time step, average out the rewards over 50 instances of the environment. Draw a plot (e.g., using Matplotlib) of average rewards received vs. time step. Do this for all agents and plot it in the same plot. Now you can compare different agents (i.e., different strategies). Since different agents have different hyper-parameters, play with different settings and you can generate multiple different plots so as to aid comparison. Analyze the plots, write about your key observations, e.g., what strategy works\n",
        "better, what settings for a strategy works better, what your findings about the agents, etc."
      ],
      "metadata": {
        "id": "aky6e4bZZvBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment_bernoulli(agent_func, agent_params, num_envs=50, steps=1000):\n",
        "    \"\"\"\n",
        "    Runs an agent  across multiple 2-armed Bernoulli bandit environments and\n",
        "    returns the average reward per time step.\n",
        "\n",
        "    Args:\n",
        "        agent_func: Agent function from Question 3.\n",
        "        agent_params: Dictionary of agent hyperparameters.\n",
        "        num_envs: Number of environments.\n",
        "        steps: Number of steps per environment.\n",
        "\n",
        "    Returns:\n",
        "        avg_rewards: Array of average rewards.\n",
        "\n",
        "    NOTE: increase the num_envs to get the much smoother curve. maybe 500 or 1000\n",
        "    \"\"\"\n",
        "    # Write your code here\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "06JH3p8WnYd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_agent_comparison(results_dict):\n",
        "    \"\"\"\n",
        "    Plots average reward vs time step for multiple agents. (in single graph)\n",
        "\n",
        "    Args:\n",
        "        results_dict: Dictionary mapping agent name -> avg reward array.\n",
        "    \"\"\"\n",
        "    # Write your code here\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "HS9zeKv7pDE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Define the agents and hyperparameters you want to compare\n",
        "'''\n",
        "agents_to_test = [\n",
        "    {\n",
        "        \"name\": \"Greedy\",\n",
        "        \"func\": pure_exploitation,\n",
        "        \"params\": {}\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Pure Exploration\",\n",
        "        \"func\": pure_exploration,\n",
        "        \"params\": {}\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Epsilon-Greedy\",\n",
        "        \"func\": epsilon_greedy,\n",
        "        \"params\": {\"epsilon\": 0.1}\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Decaying Epsilon-Greedy\",\n",
        "        \"func\": decaying_epsilon_greedy,\n",
        "        \"params\": {\"epsilon_start\": 1.0, \"epsilon_end\": 0.01, \"decay_type\": \"exponential\"}\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Softmax (Decaying)\",\n",
        "        \"func\": softmax_strategy,\n",
        "        \"params\": {\"temp_start\": 100, \"temp_end\": 0.01, \"decay_type\": \"linear\"}\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"UCB\",\n",
        "        \"func\": ucb_strategy,\n",
        "        \"params\": {\"c\": 0.5}\n",
        "    }\n",
        "]\n",
        "'''\n",
        "\n",
        "# TODO: Loop through 'agents_to_test':\n",
        "#   1. Call run_experiment_bernoulli for each agent.\n",
        "#   2. Store the resulting curve in a dictionary (e.g., experiment_results_bernoulli).\n",
        "\n",
        "# TODO: Call plot_agent_comparison(experiment_results_bernoulli).\n",
        "\n",
        "'''\n",
        "You must run the experiment multiple times with different configurations, diff agents_to_test, for example:\n",
        "\n",
        "Different values of epsilon for epsilon-greedy (e.g., 0.01, 0.1, 0.5)\n",
        "\n",
        "Different decay types (linear vs exponential)\n",
        "\n",
        "Different UCB values (e.g., c = 0.2, 0.5, 1.0)\n",
        "\n",
        "Different Softmax temperatures\n",
        "'''"
      ],
      "metadata": {
        "id": "wd-OaXdHrRNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "[Write your observations about the question and its solution here]"
      ],
      "metadata": {
        "id": "e352MgQ2Fh80"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. 10-armed bandit testbed (20 points)\n",
        "Repeat the same thing as above for 10-armed Gaussian Bandit."
      ],
      "metadata": {
        "id": "Da-VilDUaF8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment_gaussian(agent_func, agent_params,\n",
        "                                       num_envs=50, steps=1000, sigma=1.0):\n",
        "    \"\"\"\n",
        "    Runs an agent on multiple 10-armed Gaussian bandit environments and\n",
        "    returns the average reward per time step.\n",
        "\n",
        "    Args:\n",
        "        agent_func: Agent function from Question 3.\n",
        "        agent_params: Dictionary of agent hyperparameters.\n",
        "        num_envs: Number of environments.\n",
        "        steps: Number of steps per environment.\n",
        "        sigma: Reward noise standard deviation.\n",
        "\n",
        "    Returns:\n",
        "        avg_rewards: Array of average rewards (length = steps).\n",
        "\n",
        "    NOTE: increase the num_envs to get the much smoother curve. maybe 500 or 1000\n",
        "    \"\"\"\n",
        "\n",
        "    # Write your code here\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "WfG1PuCnuuAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_agent_comparison(results_dict):\n",
        "    \"\"\"\n",
        "    same as Q4\n",
        "    Plots average reward vs time step for multiple agents. (in single graph)\n",
        "\n",
        "    Args:\n",
        "        results_dict: Dictionary mapping agent name -> avg reward array.\n",
        "    \"\"\"\n",
        "    # TODO:\n",
        "    # 1. Create a matplotlib figure\n",
        "    # 2. For each agent:\n",
        "    #    - Plot average rewards vs time step\n",
        "    #    - Use agent name as label\n",
        "    # 3. Add title, axis labels, legend, and grid\n",
        "    # 4. Show the plot\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "q1Z_NRtYxwjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Define the agents and hyperparameters you want to compare\n",
        "'''\n",
        "agents_to_test = [\n",
        "    {\n",
        "        \"name\": \"Greedy\",\n",
        "        \"func\": pure_exploitation,\n",
        "        \"params\": {}\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Pure Exploration\",\n",
        "        \"func\": pure_exploration,\n",
        "        \"params\": {}\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Epsilon-Greedy\",\n",
        "        \"func\": epsilon_greedy,\n",
        "        \"params\": {\"epsilon\": _____}\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Decaying Epsilon-Greedy\",\n",
        "        \"func\": decaying_epsilon_greedy,\n",
        "        \"params\": {\"epsilon_start\": _____, \"epsilon_end\": _____, \"decay_type\": _____}\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Softmax (Decaying)\",\n",
        "        \"func\": softmax_strategy,\n",
        "        \"params\": {\"temp_start\": _____, \"temp_end\": _____, \"decay_type\": _____}\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"UCB\",\n",
        "        \"func\": ucb_strategy,\n",
        "        \"params\": {\"c\": 0.5}\n",
        "    }\n",
        "]\n",
        "'''\n",
        "\n",
        "# TODO: Loop through 'agents_to_test':\n",
        "#   1. Call run_experiment_gaussian for each agent.\n",
        "#   2. Store the resulting curve in a dictionary (e.g., experiment_results_gaussian).\n",
        "\n",
        "# TODO: Call plot_agent_comparison(experiment_results_gaussian).\n",
        "\n",
        "'''\n",
        "You must run the experiment multiple times with different configurations, diff agents_to_test, for example:\n",
        "\n",
        "Different values of epsilon for epsilon-greedy (e.g., 0.01, 0.1, 0.5)\n",
        "\n",
        "Different decay types (linear vs exponential)\n",
        "\n",
        "Different UCB values (e.g., c = 0.2, 0.5, 1.0)\n",
        "\n",
        "Different Softmax temperatures\n",
        "'''"
      ],
      "metadata": {
        "id": "eaTvpqScxgnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "[Write your observations about the question and its solution here]"
      ],
      "metadata": {
        "id": "ksiYLNSJyavI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. 2-armed Regret (20 points)\n",
        "In the lecture we defined *Regret* as $\\sum_{e=1}^{E} \\mathbb{E}[v_* - q_*(A_e)]$, i.e. episode wise sum of the expected value of the difference between optimal action value ($v_*$) and true value ($q_*$) of taking an action $A_e$. For the 2-armed Bernoulli Bandit plot the regret vs episodes for each of the agent. Use the same setting of 50 environment instances as in the previous part. What do you observe? How do regrets evolve for different methods? Describe your observations in details.\n",
        "\n"
      ],
      "metadata": {
        "id": "zRNg5H59aZIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_regret_experiment_bernoulli(agent_func, agent_params,\n",
        "                                    num_envs=50, steps=1000):\n",
        "    \"\"\"\n",
        "    Runs a bandit agent on multiple 2-armed Bernoulli environments and\n",
        "    computes the average cumulative regret.\n",
        "\n",
        "    Args:\n",
        "        agent_func: Agent function (from Question 3).\n",
        "        agent_params: Dictionary of agent hyperparameters.\n",
        "        num_envs: Number of environments to average over.\n",
        "        steps: Number of episodes per environment.\n",
        "\n",
        "    Returns:\n",
        "        avg_cumulative_regret: Cumulative expected regret (length = steps).\n",
        "    \"\"\"\n",
        "    # Write your code here\n",
        "    pass\n",
        "\n"
      ],
      "metadata": {
        "id": "UVVIHkjB3Ot_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_regret_comparison(results_dict):\n",
        "    \"\"\"\n",
        "    Plots cumulative regret vs episodes for multiple agents.\n",
        "\n",
        "    Args:\n",
        "        results_dict: Mapping agent name -> cumulative regret array.\n",
        "    \"\"\"\n",
        "    # Write your code here\n",
        "    pass"
      ],
      "metadata": {
        "id": "MOhT8pGx3Wvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Use the same setting of 50 environment\n",
        "instances as in the previous part. What do you observe? How do regrets evolve for different methods?\n",
        "Describe your observations in details.\n",
        "'''"
      ],
      "metadata": {
        "id": "Abu-ue813YPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "[Write your observations about the question and its solution here]"
      ],
      "metadata": {
        "id": "Ai0h3b6Ef1sQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. 10-armed Regret (20 points)\n",
        "\n",
        "Repeat the same thing as above for 10-armed Gaussian Bandit.\n",
        "\n"
      ],
      "metadata": {
        "id": "VJDpFbBCacXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_regret_experiment_gaussian(agent_func, agent_params,\n",
        "                                   num_envs=50, steps=1000):\n",
        "    \"\"\"\n",
        "    Runs a bandit agent on multiple 10-armed Gaussian environments and\n",
        "    computes the average cumulative regret.\n",
        "\n",
        "    Args:\n",
        "        agent_func: Agent function.\n",
        "        agent_params: Dictionary of agent hyperparameters.\n",
        "        num_envs: Number of environments to average over.\n",
        "        steps: Number of episodes per environment.\n",
        "\n",
        "    Returns:\n",
        "        avg_cumulative_regret: Cumulative expected regret (length = steps).\n",
        "    \"\"\"\n",
        "    # Write your code here\n",
        "    pass"
      ],
      "metadata": {
        "id": "dhyPCU9Badlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_regret_comparison(results_dict):\n",
        "    \"\"\"\n",
        "    same as q6\n",
        "    Plots cumulative regret vs episodes for multiple agents.\n",
        "\n",
        "    Args:\n",
        "        results_dict: Mapping agent name -> cumulative regret array.\n",
        "    \"\"\"\n",
        "    # Write your code here\n",
        "    pass"
      ],
      "metadata": {
        "id": "lqiDvstt52uD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Use the same setting of 50 environment\n",
        "instances as in the previous part. What do you observe? How do regrets evolve for different methods?\n",
        "Describe your observations in details.\n",
        "'''"
      ],
      "metadata": {
        "id": "LESEAxG96xcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**"
      ],
      "metadata": {
        "id": "ke68YH7mf8ss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![plot](https://i.postimg.cc/k4NtGvZP/Screenshot-2026-01-18-at-2-55-16-PM.png)"
      ],
      "metadata": {
        "id": "o-FCSgVXbvEq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Reward vs episodes for all agents (20 points)\n",
        "\n",
        "Plot average rewards vs episodes for each of the 6 agents in the same plot. Basically, for each agent, run 50 instances of 2-armed Bernoulli Bandit environment, run the agent for 1000 episodes; calculate the average reward at each step across all 50 instances and plot the average reward vs episodes. Just to give an example of the plot see the top plot in Figure 2.\n"
      ],
      "metadata": {
        "id": "e9y3UZp1aeLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "You may have already implemented the required experimental setup in a previous question.\n",
        "In your observations, explicitly mention: Which previous question this experiment overlaps with if it does.\n",
        "\n",
        "If you believe this experiment is not identical to what you implemented earlier,\n",
        "clearly explain why and provide the necessary code. Feel free to add the code cells below.\n",
        "'''"
      ],
      "metadata": {
        "id": "paZ7X1ZBi1SB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "[Write your observations about the question and its solution here]"
      ],
      "metadata": {
        "id": "pOnAGzdBi2vI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Optimal Action % vs episode for all agents (20 points)\n",
        "Plot % Optimal Action vs episodes for each of the 6 agents in the same plot. Basically, for each agent, run 50 instances of 2-armed Bernoulli Bandit environment, run the agent for 1000 episodes; calculate on an average (across 50 instances) for each episode how often (in %) the agent selects the optimal action and plot % Optimal Action vs episodes. Just to give an example of the plot see the bottom plot in Figure 2.\n",
        "\n"
      ],
      "metadata": {
        "id": "Joa-QKZ0agGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_optimal_action_experiment_bernoulli(agent_func, agent_params,\n",
        "                                            num_envs=50, steps=1000):\n",
        "    \"\"\"\n",
        "    Runs an agent on multiple 2-armed Bernoulli bandit environments and\n",
        "    computes the percentage of optimal actions selected per episode.\n",
        "\n",
        "    Args:\n",
        "        agent_func: Bandit agent function.\n",
        "        agent_params: Dictionary of agent hyperparameters.\n",
        "        num_envs: Number of environments to average over.\n",
        "        steps: Number of episodes per environment.\n",
        "\n",
        "    Returns:\n",
        "        pct_optimal_action: Percentage of optimal action per episode (length = steps).\n",
        "    \"\"\"\n",
        "    # Write your code here\n",
        "    pass\n",
        "\n"
      ],
      "metadata": {
        "id": "VLd-NnGrahdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_optimal_action_comparison(results_dict):\n",
        "    \"\"\"\n",
        "    Plots % Optimal Action vs Episodes for multiple agents.\n",
        "\n",
        "    Args:\n",
        "        results_dict: Mapping agent name -> % optimal action array.\n",
        "    \"\"\"\n",
        "    # Write your code here\n",
        "    pass"
      ],
      "metadata": {
        "id": "Xwdq9v-c_lIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Define the agents and hyperparameters you want to compare\n",
        "# NOTE: set num_envs=500 to see smoother curve!\n",
        "'''\n",
        "agents_to_test = [\n",
        "    {\n",
        "        \"name\": \"Greedy\",\n",
        "        \"func\": pure_exploitation,\n",
        "        \"params\": {}\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Pure Exploration\",\n",
        "        \"func\": pure_exploration,\n",
        "        \"params\": {}\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Epsilon-Greedy\",\n",
        "        \"func\": epsilon_greedy,\n",
        "        \"params\": {\"epsilon\": _____}\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Decaying Epsilon-Greedy\",\n",
        "        \"func\": decaying_epsilon_greedy,\n",
        "        \"params\": {\"epsilon_start\": _____, \"epsilon_end\": _____, \"decay_type\": _____}\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Softmax (Decaying)\",\n",
        "        \"func\": softmax_strategy,\n",
        "        \"params\": {\"temp_start\": _____, \"temp_end\": _____, \"decay_type\": _____}\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"UCB\",\n",
        "        \"func\": ucb_strategy,\n",
        "        \"params\": {\"c\": _____}\n",
        "    }\n",
        "]\n",
        "'''\n",
        "\n",
        "# TODO: Loop through 'agents_to_test':\n",
        "#   1. Call run_optimal_action_experiment_bernoulli for each agent.\n",
        "#   2. Store the resulting curve in a dictionary (e.g., optimal_action_results_bernoulli).\n",
        "\n",
        "# TODO: Call plot_optimal_action_comparison(optimal_action_results_bernoulli).\n"
      ],
      "metadata": {
        "id": "zhOmibxjCpvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "[Write your observations about the question and its solution here]"
      ],
      "metadata": {
        "id": "2wM6xvngD-lv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plots for 10-armed bandit (20 points)\n",
        "Repeat the above for 10-armed Gaussian Bandit.\n",
        "- Average reward vs episodes for all the agents in the 10-armed bandit setting.\n",
        "- Optimal action % vs episodes for all the agents in the 10-armed bandit setting."
      ],
      "metadata": {
        "id": "EjoYOF5mai_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def run_optimal_action_experiment_gaussian(agent_func, agent_params,\n",
        "                                           num_envs=50, steps=1000):\n",
        "    \"\"\"\n",
        "    Runs an agent on multiple 10-armed Gaussian bandit environments and\n",
        "    computes the percentage of optimal actions selected per episode.\n",
        "\n",
        "    Args:\n",
        "        agent_func: Bandit agent function.\n",
        "        agent_params: Dictionary of agent hyperparameters.\n",
        "        num_envs: Number of environments to average over.\n",
        "        steps: Number of episodes per environment.\n",
        "\n",
        "    Returns:\n",
        "        pct_optimal_action: Percentage of optimal action per episode (length = steps).\n",
        "    \"\"\"\n",
        "    # Write your code here\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "CD4_TxfdajnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_optimal_action_comparison(results_dict):\n",
        "    \"\"\"\n",
        "    Plots % Optimal Action vs Episodes for multiple agents.\n",
        "\n",
        "    Args:\n",
        "        results_dict: Mapping agent name -> % optimal action array.\n",
        "    \"\"\"\n",
        "    # Write your code here\n",
        "    pass"
      ],
      "metadata": {
        "id": "66A4XySBEFA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Define the agents and hyperparameters you want to compare\n",
        "# NOTE: set num_envs=500 to see smoother curve!\n",
        "'''\n",
        "agents_to_test = [\n",
        "    {\n",
        "        \"name\": \"Greedy\",\n",
        "        \"func\": pure_exploitation,\n",
        "        \"params\": {}\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Pure Exploration\",\n",
        "        \"func\": pure_exploration,\n",
        "        \"params\": {}\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Epsilon-Greedy\",\n",
        "        \"func\": epsilon_greedy,\n",
        "        \"params\": {\"epsilon\": _____}\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Decaying Epsilon-Greedy\",\n",
        "        \"func\": decaying_epsilon_greedy,\n",
        "        \"params\": {\"epsilon_start\": _____, \"epsilon_end\": _____, \"decay_type\": _____}\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Softmax (Decaying)\",\n",
        "        \"func\": softmax_strategy,\n",
        "        \"params\": {\"temp_start\": _____, \"temp_end\": _____, \"decay_type\": _____}\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"UCB\",\n",
        "        \"func\": ucb_strategy,\n",
        "        \"params\": {\"c\": _____}\n",
        "    }\n",
        "]\n",
        "'''\n",
        "\n",
        "# TODO: Loop through 'agents_to_test':\n",
        "#   1. Call run_optimal_action_experiment_gaussian for each agent.\n",
        "#   2. Store the resulting curve in a dictionary (e.g., optimal_action_results_gaussian).\n",
        "\n",
        "# TODO: Call plot_optimal_action_comparison(optimal_action_results_gaussian).\n"
      ],
      "metadata": {
        "id": "VlibJnaHD-JX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "[Write your observations about the question and its solution here]"
      ],
      "metadata": {
        "id": "VKpifL3qEfNQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 2: Monte Carlo Estimates and TD Learning\n",
        "($10+10+40+40+20+20+20+20+20+20+10+20+20+20+10=300$ points)\n",
        "\n",
        "In lecture 7, we saw the **Random Walk Environment (RWE)**. Figure 3 is the RWE for reference, please refer to the lecture for more details. In a nutshell, there are 5 non-terminal states, and two terminal states. An action leading to state 6 gives a reward of +1 and rest all actions give a reward of 0. The environment is purely random, no matter what action you take there is 50% chance that it will be executed as intended and rest 50% times it goes in opposite direction.\n",
        "\n",
        "![Random Walk Environment](https://i.postimg.cc/brNB9nrP/Whats-App-Image-2026-01-22-at-1-50-50-PM.jpg)\n",
        "\n",
        "\n",
        "In lecture 7, we learnt about Monte Carlo method and TD learning method for estimating the value of a state:\n",
        "\n",
        "**Monte Carlo Estimate:**\n",
        "$$\n",
        "V_{e+1}(s) = V_{e}(s) + \\alpha(e) [G_{e} - V_{e}(s)]\n",
        "$$\n",
        "\n",
        "**TD Learning Estimate:**\n",
        "$$\n",
        "V_{t+1}(S_{t}) = V_{t}(S_{t}) + \\alpha(t) [R_{t+1} + \\gamma V_{t}(S_{t+1}) - V_{t}(S_{t})]\n",
        "$$\n",
        "\n",
        "In this problem, you are required to calculate Monte Carlo and TD estimates for the RWE. Assume $\\gamma=0.99$, policy ($\\pi$) is \"go left\"."
      ],
      "metadata": {
        "id": "KHbM7FEspjIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prerequisites & Environment Setup\n",
        "First, we define the **Random Walk Environment (RWE)** and the helper function to calculate the **True Values** of the states."
      ],
      "metadata": {
        "id": "-VMBkOnDoCbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(40)\n",
        "\n",
        "# --- Random Walk Environment ---\n",
        "class RandomWalkEnv:\n",
        "    \"\"\"\n",
        "    7-state Random Walk Environment.\n",
        "    States: 0 (Terminal), 1, 2, 3, 4, 5, 6 (Terminal).\n",
        "    Start State: 3.\n",
        "    Actions: 0 (Left), 1 (Right). (Environment ignores action due to 50% randomness)\n",
        "    Rewards: +1 when transitioning to state 6, 0 otherwise.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.observation_space = [_, _, _, _, _, _, _]\n",
        "        self.start_state = _\n",
        "        self.current_state = _\n",
        "        self.end_states = [_, _]\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_state = self.start_state\n",
        "        return self.current_state\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Executes one time step within the environment.\n",
        "\n",
        "        Args:\n",
        "            action (int): The selected action (0 for Left, 1 for Right).\n",
        "                          Note: The environment is random (50/50), so the specific action is often ignored in logic.\n",
        "\n",
        "        Returns:\n",
        "            next_state (int): The state after the transition.\n",
        "            reward (float): The reward received (1 if reaching state 6, 0 otherwise).\n",
        "            done (bool): True if the episode has ended (reached state 0 or 6), False otherwise.\n",
        "            info (dict): Additional information (empty dictionary).\n",
        "        \"\"\"\n",
        "        # Write your code here\n",
        "        pass\n",
        "\n",
        "# --- True Value Calculation (Helper) ---\n",
        "def get_true_values(gamma=0.99):\n",
        "    \"\"\"\n",
        "    Solves the Bellman system of equations for the random walk.\n",
        "    V(s) = 0.5*[R_left + gamma*V(s-1)] + 0.5*[R_right + gamma*V(s+1)]\n",
        "    \"\"\"\n",
        "    # States 1, 2, 3, 4, 5 are variables.\n",
        "    A = __\n",
        "    b = __\n",
        "    v = __\n",
        "    # Map back to full states 0..6 (Terminal states are 0)\n",
        "    pass\n",
        "\n",
        "TRUE_VALUES = get_true_values()\n",
        "print(\"True Values for States 0-6:\", np.round(TRUE_VALUES, 3))"
      ],
      "metadata": {
        "id": "KseUsBBcoNLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Generate trajectory (10 points)\n",
        "Implement a function that would simulate and generate a trajectory for RWE for a given policy $\\pi$ and maximum number of steps. The function definition would be like this:"
      ],
      "metadata": {
        "id": "1ayTp3A8pul7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generateTrajectory(env, pi, maxSteps):\n",
        "    \"\"\"\n",
        "    Simulates and generates a trajectory for the Random Walk Environment.\n",
        "\n",
        "    Args:\n",
        "        env (RandomWalkEnv): The environment instance.\n",
        "        pi (int): The policy to follow (e.g., 0 for \"go left\").\n",
        "        maxSteps (int): Maximum number of steps to run the episode.\n",
        "\n",
        "    Returns:\n",
        "        trajectory (list): A list of experience tuples, where each tuple is (state, action, reward, next_state).\n",
        "                           Returns an empty list if the episode exceeds maxSteps without terminating.\n",
        "    \"\"\"\n",
        "    # Write your code here\n",
        "    pass\n",
        "\n",
        "# Test Case\n",
        "# env = RandomWalkEnv()\n",
        "# test_traj = generateTrajectory(env, 0, 100)\n",
        "# print(f\"Sample Trajectory (Length {len(test_traj)}):\", test_traj)"
      ],
      "metadata": {
        "id": "3Kk0wZRLp7mT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function returns a list of experience tuples. Here, `maxSteps` parameter is used to terminate the episode if it exceeds `maxSteps` count. In such a case, the partial trajectory is discarded and an empty list is returned. Test the function using suitable test cases and make sure it is working."
      ],
      "metadata": {
        "id": "_R4bpklRp_QY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Step-size decay (10 points)\n",
        "Implement a function that would decay the step size parameter ($\\alpha$). The function definition would be like this:"
      ],
      "metadata": {
        "id": "avuLSC6YqCUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decayAlpha(initialValue, finalValue, maxSteps, decayType):\n",
        "    \"\"\"\n",
        "    Generates a list of alpha values (step sizes) based on the specified decay type.\n",
        "\n",
        "    Args:\n",
        "        initialValue (float): The starting value of alpha.\n",
        "        finalValue (float): The final value of alpha.\n",
        "        maxSteps (int): The total number of steps/episodes to decay over.\n",
        "        decayType (str): The type of decay ('linear' or 'exponential').\n",
        "\n",
        "    Returns:\n",
        "        alphas (np.ndarray): An array of alpha values for each step.\n",
        "    \"\"\"\n",
        "    # Write your code here\n",
        "    pass"
      ],
      "metadata": {
        "id": "Lrb9rEttqJvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here `decayType` can be `linear` or `exponential`. `maxSteps` is the maximum number of steps the step parameter should decay for. `initialValue` and `finalValue` are initial and final values of the step size parameter. The function should return a list of step size parameter values. Test the function by trying out different parameter settings. Plot value of $\\alpha$ vs time step both for linear and exponential decays."
      ],
      "metadata": {
        "id": "eqPIB_kPqN3p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Monte Carlo Prediction (40 points)\n",
        "As explained in the lecture, implement `MonteCarloPrediction` algorithm. Use the same function definition as described in the slides. Make use of the functions implemented in above two parts. Note `MonteCarloPrediction` should work for both **FVMC** (First Visit MC) and **EVMC** (Every Visit MC) settings. Test the algorithm for RWE using some pre-defined test cases and see the algorithm produces the desired results. Report your test cases and observations."
      ],
      "metadata": {
        "id": "7y6Kph_6pvPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def monte_carlo_prediction(env, num_episodes, alpha_start=0.5, alpha_end=0.01,\n",
        "                           decay_period=250, mc_type='FVMC', gamma=0.99):\n",
        "    \"\"\"\n",
        "    Implements the Monte Carlo Prediction algorithm (works for both FVMC and EVMC).\n",
        "\n",
        "    Args:\n",
        "        env (RandomWalkEnv): The environment instance.\n",
        "        num_episodes (int): Total number of episodes to simulate.\n",
        "        alpha_start (float): Initial step size (learning rate).\n",
        "        alpha_end (float): Final step size.\n",
        "        decay_period (int): Number of episodes over which alpha decays.\n",
        "        mc_type (str): Type of MC update - 'FVMC' (First Visit) or 'EVMC' (Every Visit).\n",
        "        gamma (float): Discount factor.\n",
        "\n",
        "    Returns:\n",
        "        V_history (np.ndarray): History of value estimates for each state over all episodes.\n",
        "                                Shape: [num_episodes, n_states]\n",
        "    \"\"\"\n",
        "    # Write your code here\n",
        "    pass"
      ],
      "metadata": {
        "id": "CnW0i-RipbbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Temporal Difference Prediction (40 points)\n",
        "\n",
        "As explained in the lecture, implement `TemporalDifferencePrediction` algorithm. Use the same function definition as described in the slides. Test the algorithm for RWE using some pre-defined test cases and see the algorithm produces the desired results. Report your test cases and observations."
      ],
      "metadata": {
        "id": "CnjiXWwsqRIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def temporal_difference_prediction(env, num_episodes, alpha_start=0.5, alpha_end=0.01,\n",
        "                                   decay_period=250, gamma=0.99):\n",
        "    \"\"\"\n",
        "    Implements the Temporal Difference TD(0) Prediction algorithm.\n",
        "\n",
        "    Args:\n",
        "        env (RandomWalkEnv): The environment instance.\n",
        "        num_episodes (int): Total number of episodes to simulate.\n",
        "        alpha_start (float): Initial step size.\n",
        "        alpha_end (float): Final step size.\n",
        "        decay_period (int): Number of episodes over which alpha decays.\n",
        "        gamma (float): Discount factor.\n",
        "\n",
        "    Returns:\n",
        "        V_history (np.ndarray): History of value estimates for each state over all episodes.\n",
        "                                Shape: [num_episodes, n_states]\n",
        "    \"\"\"\n",
        "    # Write your code here\n",
        "    pass"
      ],
      "metadata": {
        "id": "7qpznL9DrYdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. First-Visit Monte Carlo estimate (20 points)\n",
        "Plot the **MC-FVMC** estimate of each non-terminal state of RWE as it progress through different episodes. In the same plot also plot the true estimate. Take maximum of 500 episodes. You can play with different settings of $\\alpha$, for example, the step size parameter ($\\alpha$) starts from 0.5 and decreases exponentially to 0.01 till 250 episodes and after that it is constant. Or else you can also try with small $(<1)$ value of constant $\\alpha$. Analyze the plots for each state and report your observations, findings and possible reasons for the observed behavior."
      ],
      "metadata": {
        "id": "m8w2_2eQqXY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_value_estimates(V_history, true_values, title, log_scale=False):\n",
        "    \"\"\"\n",
        "    Plots the estimated value of non-terminal states (1-5) over episodes against the true values.\n",
        "\n",
        "    Args:\n",
        "        V_history (np.ndarray): The history of value estimates. Shape: [num_episodes, n_states].\n",
        "        true_values (np.ndarray): The ground truth values for the states.\n",
        "        title (str): Title for the plot.\n",
        "        log_scale (boolean): If True, use a logarithmic scale for the y-axis.\n",
        "\n",
        "    Returns:\n",
        "        None (Displays the plot).\n",
        "    \"\"\"\n",
        "    # Write your code here\n",
        "    pass\n",
        "\n",
        "# Example Usage for Q5 (MC-FVMC)\n",
        "# history_fvmc = monte_carlo_prediction(env, 500, mc_type='FVMC')\n",
        "# plot_value_estimates(history_fvmc, TRUE_VALUES, \"MC-FVMC Estimates\")"
      ],
      "metadata": {
        "id": "DchHIAD6p7Np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9aUM8tRnrZSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations (MC-FVMC):**\n",
        "\n",
        "[Write your observations about the question and its solution here]"
      ],
      "metadata": {
        "id": "oWayg7Hko9Eq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Every Visit Monte Carlo estimate (20 points)\n",
        "Plot the **MC-EVMC** estimate of each non-terminal state of RWE as it progress through different episodes. In the same plot also plot the true estimate. Take maximum of 500 episodes. You can play with different settings of $\\alpha$, for example, the step size parameter ($\\alpha$) starts from 0.5 and decreases exponentially to 0.01 till 250 episodes and after that it is constant. Or else you can also try with small $(<1)$ value of constant $\\alpha$. Analyze the plots for each state and report your observations, findings and possible reasons for the observed behavior. How does EVMC fair against FVMC?"
      ],
      "metadata": {
        "id": "Ddl5hXy8qjLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement Every-Visit Monte Carlo prediction\n",
        "# Hint: Use mc_type = 'EVMC'\n",
        "\n",
        "num_episodes = 500\n",
        "\n",
        "# Example Usage for Q6 (MC-EVMC)\n",
        "# TODO: Call Monte Carlo prediction function\n",
        "# Store value estimates across episodes\n",
        "# history_evmc = monte_carlo_prediction(env, 500, mc_type='EVMC')\n",
        "\n",
        "# TODO: Plot the values\n",
        "# plot_value_estimates(history_evmc, TRUE_VALUES, \"MC-EVMC Estimates\")\n",
        "\n"
      ],
      "metadata": {
        "id": "qWWauh5GraBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations (MC-EVMC):**\n",
        "\n",
        "[Write your observations about the question and its solution here]"
      ],
      "metadata": {
        "id": "m5P3ME_hpCj-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. TD esitmate (20 points)\n",
        "Plot the **TD** estimate of each non-terminal state of RWE as it progress through different episodes. In the same plot also plot the true estimate. Take maximum of 500 episodes. You can play with different settings of $\\alpha$, for example, the step size parameter ($\\alpha$) starts from 0.5 and decreases exponentially to 0.01 till 250 episodes and after that it is constant. Or else you can also try with small $(<1)$ value of constant $\\alpha$. Analyze the plots for each state and report your observations, findings and possible reasons for the observed behavior."
      ],
      "metadata": {
        "id": "PqPEHfc0qdIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement Temporal Difference (TD(0)) prediction\n",
        "\n",
        "num_episodes = 500\n",
        "\n",
        "# Example Usage for Q7 (TD)\n",
        "# TODO: Call TD prediction function\n",
        "# Store value estimates across episodes\n",
        "# history_td = temporal_difference_prediction(env, 500)\n",
        "\n",
        "# TODO: Plot the values\n",
        "# plot_value_estimates(history_td, TRUE_VALUES, \"TD Estimates\")\n"
      ],
      "metadata": {
        "id": "iKXLrZ0NrbV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations (TD):**\n",
        "\n",
        "[Write your observations about the question and its solution here]"
      ],
      "metadata": {
        "id": "1RYWwfw5pHP-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Smoothing variation (20 points)\n",
        "As you might notice, the above plots may have lot of variance and look very noisy. One way to overcome this is to create several different instances of the environment using different seeds and then average out the results across these and plot these (this is similar to the Bandit setting). Plot MC-FVMC, MC-EVMC and TD with this averaged out version. This will give you smoother plots. The plot will be similar to shown in lecture 7. Record your seeds and report these along with the plots."
      ],
      "metadata": {
        "id": "leOHGyWnqpNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Average value estimates over multiple runs\n",
        "# Methods: MC-FVMC, MC-EVMC, TD\n",
        "\n",
        "num_runs = 50\n",
        "num_episodes = 500\n",
        "\n",
        "\n",
        "def run_averaged_experiment(agent_func, env, num_runs=50, num_episodes=500, **kwargs):\n",
        "    \"\"\"\n",
        "    Runs an RL agent/algorithm multiple times on a given environment to compute\n",
        "    averaged value estimates. Each run uses a different random seed for reproducibility.\n",
        "\n",
        "    Args:\n",
        "        agent_func (callable): The agent function to execute (e.g., monte_carlo_prediction,\n",
        "                               temporal_difference_prediction).\n",
        "                               Expected signature: func(env, num_episodes, **kwargs) -> V\n",
        "        env (gym.Env): The environment instance to run the experiment on.\n",
        "        num_runs (int, optional): Number of independent runs to perform. Defaults to 50.\n",
        "        num_episodes (int, optional): Number of episodes per run. Defaults to 500.\n",
        "        **kwargs: Arbitrary keyword arguments passed directly to the agent_func\n",
        "                  (e.g., mc_type='FVMC', alpha=0.1, gamma=0.99).\n",
        "\n",
        "    Returns:\n",
        "        avg_estimates (np.ndarray): The estimated value function averaged over `num_runs`.\n",
        "                                    Shape should correspond to the number of non-terminal states.\n",
        "        seeds (list of int): A list of the random seeds used for each run.\n",
        "    \"\"\"\n",
        "    # Write your code here\n",
        "    pass\n",
        "\n",
        "\n",
        "\n",
        "# Run Averaged Experiments\n",
        "avg_fvmc, seeds_fvmc = run_averaged_experiment(monte_carlo_prediction, env, mc_type='FVMC')\n",
        "avg_evmc, seeds_evmc = run_averaged_experiment(monte_carlo_prediction, env, mc_type='EVMC')\n",
        "avg_td, seeds_td = run_averaged_experiment(temporal_difference_prediction, env)\n",
        "\n",
        "print(\"Averaging complete over 50 runs.\")\n",
        "print(\"Sample Seeds (TD):\", seeds_td[:5])\n",
        "\n",
        "# Plot all three averaged\n",
        "plot_value_estimates(avg_fvmc, 'Average MC-FVMC (50 Runs)', TRUE_VALUES)\n",
        "plot_value_estimates(avg_evmc, 'Average MC-EVMC (50 Runs)', TRUE_VALUES)\n",
        "plot_value_estimates(avg_td, 'Average TD (50 Runs)', TRUE_VALUES)\n",
        "\n",
        "# TODO: Compute averaged estimates for all methods\n"
      ],
      "metadata": {
        "id": "KV0zRc2Nrb-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "[Write your observations about the question and its solution here]"
      ],
      "metadata": {
        "id": "Du_JV60cLObN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. FVMC estimate in log-scale (20 points)\n",
        "Plot the **MC-FVMC** estimate of each non-terminal state of RWE as it progress through different episodes. But this time, the **x-axis (episodes) should be log-scale**. In the same plot also plot the true estimate. The plot will be similar to shown in lecture 7. Take maximum of 500 episodes. You can play with different settings of $\\alpha$, for example, the step size parameter ($\\alpha$) starts from 0.5 and decreases exponentially to 0.01 till 250 episodes and after that it is constant. Or else you can also try with small $(<1)$ value of constant $\\alpha$. This plot will help to zoom in and observe the behavior of the estimates in the initial stages. Analyze the plots for each state and report your observations, findings and possible reasons for the observed behavior."
      ],
      "metadata": {
        "id": "fWnIoxynqs84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Plot averaged MC-FVMC estimates using log scale\n",
        "# Example Usage for Q5 (MC-FVMC)\n",
        "history_fvmc = monte_carlo_prediction(env, 500, mc_type='FVMC')\n",
        "plot_value_estimates(history_fvmc, TRUE_VALUES, \"MC-FVMC Estimates\", log_scale=______)"
      ],
      "metadata": {
        "id": "s_K2AP3jrc0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "[Write your observations about the question and its solution here]"
      ],
      "metadata": {
        "id": "QfiN6EqTpNJf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. EVMC estimate in log-scale (20 points)\n",
        "\n",
        "Plot the **MC-EVMC** estimate of each non-terminal state of RWE as it progress through different episodes. But this time, the **x-axis (episodes) should be log-scale**. In the same plot also plot the true estimate. Take maximum of 500 episodes. You can play with different settings of $\\alpha$. This plot will help to zoom in and observe the behavior of the estimates in the initial stages. Analyze the plots for each state and report your observations, findings and possible reasons for the observed behavior. How does EVMC fair against FVMC?"
      ],
      "metadata": {
        "id": "B1GAlOgyq4LO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Plot averaged MC-EVMC estimates using log scale\n",
        "# TODO: Call Monte Carlo prediction function\n",
        "# Store value estimates across episodes\n",
        "history_evmc = monte_carlo_prediction(env, 500, mc_type='EVMC')\n",
        "\n",
        "# TODO: Plot the values\n",
        "plot_value_estimates(history_evmc, TRUE_VALUES, \"MC-EVMC Estimates\", log_scale=______)"
      ],
      "metadata": {
        "id": "k-guOWjQrdlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "[Write your observations about the question and its solution here]"
      ],
      "metadata": {
        "id": "rL4RA5C7pTtl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11. TD estimate of non-terminal states (10 points)\n",
        "\n",
        "Plot the **TD** estimate of each non-terminal state of RWE as it progress through different episodes. But this time, the **x-axis (episodes) should be log-scale**. In the same plot also plot the true estimate. The plot will be similar to shown in lecture 7. Take maximum of 500 episodes. You can play with different settings of $\\alpha$. This plot will help to zoom in and observe the behavior of the estimates in the initial stages. Analyze the plots for each state and report your observations, findings and possible reasons for the observed behavior."
      ],
      "metadata": {
        "id": "hNYiLj4xqyCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Plot averaged TD estimates for non-terminal states using log scale\n",
        "# TODO: Call TD prediction function\n",
        "# Store value estimates across episodes\n",
        "history_td = temporal_difference_prediction(env, 500)\n",
        "\n",
        "# TODO: Plot the values\n",
        "plot_value_estimates(history_td, TRUE_VALUES, \"TD Estimates\", log_scale=_______)"
      ],
      "metadata": {
        "id": "-rRM5hPgreOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations (Log Scale):**\n",
        "\n",
        "[Write your observations about the question and its solution here]"
      ],
      "metadata": {
        "id": "ZUrAc4yVpXs8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12. Comparitive analysis (20 points)\n",
        "\n",
        "Based on the plots, compare MC-FVMC, MC-EVMC and TD approaches and report your observations."
      ],
      "metadata": {
        "id": "7PEulWEyq8bZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "PMG8ZdF7MLuS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 13. FVMC Return for non-terminal state (20 points)\n",
        "Plot the **MC-FVMC Target value ($G_{t}$)** for any one non-terminal state of RWE as it progress through different episodes. Use the same setting as above. In the same plot also include the optimal value of the state. The plot will be similar to discussed in lecture 7. What do you observe and what are the reasons for what you observe? Explain and Report."
      ],
      "metadata": {
        "id": "w4h54dcjrD-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_mc_targets(env, num_episodes, state_to_track=3, mc_type='FVMC', gamma=0.99):\n",
        "    \"\"\"\n",
        "    Runs episodes and collects the Monte Carlo target values (Returns G_t)\n",
        "    for a specific state whenever it is visited.\n",
        "\n",
        "    Args:\n",
        "        env (RandomWalkEnv): The environment instance.\n",
        "        num_episodes (int): Total number of episodes to run.\n",
        "        state_to_track (int): The specific non-terminal state to track (e.g., 3).\n",
        "        mc_type (str): 'FVMC' (First-Visit) or 'EVMC' (Every-Visit).\n",
        "                       - FVMC: Collect target only for the first occurrence in an episode.\n",
        "                       - EVMC: Collect target for every occurrence in an episode.\n",
        "        gamma (float): Discount factor for calculating returns.\n",
        "\n",
        "    Returns:\n",
        "        targets (list of float): A list containing all the collected target values (G_t)\n",
        "                                 for the tracked state across all episodes.\n",
        "    \"\"\"\n",
        "    # TODO: Initialize an empty list 'targets' to store the G_t values.\n",
        "\n",
        "    # TODO: Loop over 'num_episodes':\n",
        "    #     1. Generate a trajectory using your generateTrajectory() function.\n",
        "    #     2. Initialize G = 0.\n",
        "    #     3. Loop backwards through the trajectory (from T-1 to 0):\n",
        "    #         a. Update G: G = reward + gamma * G\n",
        "    #         b. Get the state at the current step.\n",
        "    #         c. Check if this state == state_to_track.\n",
        "    #            - If True:\n",
        "    #                 - If mc_type == 'EVMC':\n",
        "    #                     Append G to 'targets'.\n",
        "    #                 - If mc_type == 'FVMC':\n",
        "    #                     Store G temporarily (or check if this is the first visit index).\n",
        "    #                     (Hint: For FVMC, you might want to find the *first* index of the state\n",
        "    #                      in the trajectory forward-pass or just overwrite the G value\n",
        "    #                      as you go backwards so you end up with the return for the first visit).\n",
        "    #\n",
        "    #     4. (For FVMC specific logic): If using the overwrite method, append the final stored G\n",
        "    #        for the first visit to 'targets'.\n",
        "\n",
        "    # TODO: Return the list of targets.\n",
        "    pass\n",
        "\n",
        "def plot_target_distribution(targets, true_value, title):\n",
        "    \"\"\"\n",
        "    Plots the target values (G_t) across episodes/occurrences.\n",
        "\n",
        "    Args:\n",
        "        targets (list): List of collected target values.\n",
        "        true_value (float): The optimal/true value of the state (for reference line).\n",
        "        title (str): Title of the plot.\n",
        "    \"\"\"\n",
        "    # Write your code here\n",
        "    pass"
      ],
      "metadata": {
        "id": "U9WOUvuw6UaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Call the collection function for First-Visit MC\n",
        "# We use 'FVMC' so we only store the return for the *first* time state 3 is visited per episode.\n",
        "targets_fvmc = collect_mc_targets(env, num_episodes=500, state_to_track=3, mc_type='FVMC')\n",
        "\n",
        "# TODO: Plot the values with the True Value line for comparison\n",
        "# The plot allows us to see the \"noise\" in the returns.\n",
        "plot_target_distribution(targets_fvmc, TRUE_VALUES[3], \"Q13: FVMC Target Distribution (State 3)\")"
      ],
      "metadata": {
        "id": "5i62NbmMrfWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 14. EVMC Return for non-terminal state (20 points)\n",
        "\n",
        "Plot the **MC-EVMC Target value ($G_{t}$)** for any one non-terminal state of RWE (use the same state as above) as it progress through different episodes. Use the same setting as above. In the same plot also include the optimal value of the state. What do you observe and what are the reasons for what you observe? Explain and Report."
      ],
      "metadata": {
        "id": "Hvts3RlrrHQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Call the collection function for Every-Visit MC\n",
        "# We use 'EVMC' to store returns for *every* occurrence of state 3.\n",
        "targets_evmc = collect_mc_targets(env, num_episodes=500, state_to_track=3, mc_type='EVMC')\n",
        "\n",
        "# TODO: Plot the values to compare against FVMC and True Value\n",
        "plot_target_distribution(targets_evmc, TRUE_VALUES[3], \"Q14: EVMC Target Distribution (State 3)\")"
      ],
      "metadata": {
        "id": "7aH4F4SSrf5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n"
      ],
      "metadata": {
        "id": "B5PX47oppgcn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 15. TD Return for non-terminal state (10 points)\n",
        "\n",
        "Plot the **TD Target value ($G_{t}$ estimate)** for any one non-terminal state of RWE (use the same state as above) as it progress through different episodes. Use the same setting as above. In the same plot also include the optimal value of the state. The plot will be similar to discussed in lecture 7. What do you observe and what are the reasons for what you observe? Explain and Report."
      ],
      "metadata": {
        "id": "Pvp5Iak-rMbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_td_targets(env, num_episodes, state_to_track=3, alpha=0.1, gamma=0.99):\n",
        "    \"\"\"\n",
        "    Runs episodes and collects the TD Target values (R + gamma * V(s'))\n",
        "    for a specific state to observe how they vary and evolve.\n",
        "\n",
        "    Args:\n",
        "        env (RandomWalkEnv): The environment instance.\n",
        "        num_episodes (int): Total number of episodes to run.\n",
        "        state_to_track (int): The specific non-terminal state to track (e.g., 3).\n",
        "        alpha (float): Step size for updating V (needed so the value estimate evolves).\n",
        "        gamma (float): Discount factor.\n",
        "\n",
        "    Returns:\n",
        "        targets (list of float): A list containing all the collected TD target values\n",
        "                                 for the tracked state across all episodes.\n",
        "    \"\"\"\n",
        "   # Write your code here\n",
        "    pass\n",
        "\n",
        "def plot_target_distribution(targets, true_value, title):\n",
        "    \"\"\"\n",
        "    Plots the specific target values across occurrences to visualize variance.\n",
        "\n",
        "    Args:\n",
        "        targets (list): List of target values collected.\n",
        "        true_value (float): The optimal value for the tracked state.\n",
        "        title (str): Plot title.\n",
        "    \"\"\"\n",
        "    # Write your code here\n",
        "    pass\n",
        "\n",
        "# Example Usage\n",
        "\n",
        "# targets_td = collect_td_targets(env, num_episodes=500, state_to_track=3, alpha=0.1)\n",
        "# plot_target_distribution(targets_td, TRUE_VALUES[3], \"TD Target Distribution (State 3)\")"
      ],
      "metadata": {
        "id": "s9FpkXC2rgb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n"
      ],
      "metadata": {
        "id": "HwmlqPqlpo9J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 16. Target Comparison\n",
        "\n",
        "Compare First-View, Every-View Monte Carlo and Temporal difference prediction algorithms on their target return values.\n"
      ],
      "metadata": {
        "id": "oYRtO7-AptDS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "nd8nokdPM8A2"
      }
    }
  ]
}