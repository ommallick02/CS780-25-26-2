{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS780: Deep Reinforcement Learning Assignment - 2\n",
        "***Instructor: Ashutosh Modi***\n",
        "\n",
        "\n",
        "- Submission Deadline: 11:59PM, February 22nd, 2026\n",
        "- Submission Link: [https://forms.gle/sVsreFguMKG1GfQD7](https://forms.gle/sVsreFguMKG1GfQD7)"
      ],
      "metadata": {
        "id": "a9im0JhryKT6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5P_nZkiHRQU_"
      },
      "source": [
        "Name: **YOUR FULL NAME GOES HERE**\n",
        "\n",
        "Roll No.: **YOUR ROLL NO**\n",
        "\n",
        "IITK EMail: **email@iitk.ac.in**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instructions\n",
        "Read all the instructions below carefully before you start working on the assignment.\n",
        "\n",
        "\n",
        "• The purpose of this course is that you learn RL and the best way to do that is by implementation and\n",
        "experimentation.\n",
        "\n",
        "• The assignment requires your to implement some algorithms and you are required report your findings\n",
        "after experimenting with those algorithms in this notebook.\n",
        "\n",
        "• Implement the code in this Google Colab Notebook and write your findings and observations in markdown cells to create a complete report.\n",
        "\n",
        "• In case you use any maths in your explanations, render it using latex in the notebook.\n",
        "\n",
        "• You are expected to implement algorithms on your own and not copy it from other sources/class mates.\n",
        "Of course, you can refer to lecture slides.\n",
        "\n",
        "• If you use any reference or material (including code), please cite the source, else it will be considered\n",
        "plagiarism. But referring to other sources that directly solve the problems given in the assignment is not\n",
        "allowed. There is a limit to which you can refer to outside material.\n",
        "\n",
        "• This is an individual assignment.\n",
        "\n",
        "• In case your solution is found to have an overlap with solution by someone else (including external sources),\n",
        "all the parties involved will get zero in this and all future assignments plus further more penalties in the\n",
        "overall grade. We will check not just for lexical but also semantic overlap. Same applies for the code as\n",
        "well. Even an iota of cheating would NOT be tolerated. If you cheat one line or cheat one page\n",
        "the penalty would be same.\n",
        "\n",
        "• Be a smart agent, think long term, if you cheat we will discover it somehow, the price you would be paying\n",
        "is not worth it.\n",
        "\n",
        "• In case you are struggling with the assignment, seek help from TAs. Cheating is not an option! I respect\n",
        "honesty and would be lenient if you are not able to solve some questions due to difficulty in understanding.\n",
        "Remember we are there to help you out, seek help if something is difficult to understand.\n",
        "\n",
        "• The deadline for the submission is given above. Submit at least 30 minutes before the deadline, lot can\n",
        "happen at the last moment, your internet can fail, there can be a power failure, you can be abducted by\n",
        "aliens, etc.\n",
        "\n",
        "• You have to submit your assignment via a Google Form (link above)\n",
        "\n",
        "• The form would close after the deadline and we will not accept any solution. No reason what-so-ever\n",
        "would be accepted for not being able to submit before the deadline.\n",
        "\n",
        "• Since the assignment involves experimentation, reporting your results and observations, there is a lot\n",
        "of scope for creativity and innovation and presenting new perspectives. Such efforts would be highly\n",
        "appreciated and accordingly well rewarded. Be an exploratory agent!\n",
        "\n",
        "• Your code should be very well documented, there are marks for that.\n",
        "\n",
        "• In your plots, have a clear legend and clear lines, etc. Of course you would generating the plots in your\n",
        "code but you must also put these plots in your notebook.\n",
        "\n",
        "• For all experiments, report about the seed used in the code documentation, write about the seed used.\n",
        "\n",
        "• In your notebook write about all things that are not obvious from the code e.g., if you have made any\n",
        "assumptions, references/sources, running time, etc.\n",
        "\n",
        "<font color = red><b>\n",
        "\n",
        "• In addition to checking your code and report, very likely, we will be conducting one-on-one\n",
        "viva for the evaluation. So please make sure that you do not cheat!\n",
        "</font></b>\n",
        "\n",
        "<font color = red><b>\n",
        "• If you use LLMs or LLM based tools (also refer to usage policy below) for coding or for any other purpose, please declare it very clearly in your solution (see the declaration form below). You need to answer questions like: What was the LLM used for? What was your contibution after the LLM usage? Any other information that shows that despite using the LLM you did make contibutions to the assignment, things like analysis, etc. Failure to declare would taken as cheating/plagiarism and will be very heavily penalized.\n",
        "</font></b>\n",
        "\n"
      ],
      "metadata": {
        "id": "-gJ4YiAryngh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# LLM Usage Policy\n",
        "\n",
        "It is **OK** to use LLM or other sources (websites, blogs, books, etc) for:\n",
        "* Coding ancillary parts of the algorithm such as plotting graphs, etc.\n",
        "* To develop understanding of the algorithm in the case it is not clear to you\n",
        "\n",
        "It is **NOT OK** to use LLM or other sources (websites, blogs, books, etc) for:\n",
        "* Writing the code for the main parts of the algorithm especially given that I have already provided pseudo-code in the lectures. (You can use pseudo code as it is from the lecture slides)\n",
        "* For doing hyper-parameter tuning of the algorithm.\n",
        "* For doing analysis of the results and drawing conclusions about it\n",
        "\n",
        "Since RL is specialized domain, it is very straight forward to detect when someone (especially a newbie) is taking code from LLM or other sources. In case you are found to do so (even for a small part of assignment), there will be no questions asked direct zero will be awarded to you along with further penalties which will affect your final grade.\n",
        "\n",
        "Be a smart agent (remember Honesty is the best Policy), it is ok to loose a few marks than getting a zero + penalties. It is OK if you don't do all questions in the assignment which will cost you a few marks vs getting 0.\n",
        "\n",
        "**Remember that grading is relative and you are not the only one who is finding it tough!**\n"
      ],
      "metadata": {
        "id": "hL1-F1B5FmK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Usage Declaration\n",
        "\n",
        "It is permitted to use Large Language Models (LLMs) **only for understanding concepts, ideation, and creating initial code stubs**.  \n",
        "Using an LLM to **completely solve the assignment is not allowed**.  \n",
        "\n",
        "Even if an LLM is used for the permitted purposes, you **must be able to clearly explain the solution without any LLM assistance during the viva**, if called.\n",
        "\n",
        "---\n",
        "\n",
        "### Q1. **Which Large Language Model (LLM) did you use to complete this assignment?**\n",
        "\n",
        "> _Your answer here_\n",
        "\n",
        "---\n",
        "\n",
        "### Q2. **In which questions did you use the LLM, and how did it help you?**  \n",
        "(You may extend this table by adding more rows if needed.)\n",
        "\n",
        "| Question Number / Sub-part | Used LLM? (Yes/No) | Purpose of Using LLM (e.g., ideation, explanation, code stub, grammar) | Brief Description of LLM Contribution |\n",
        "|----------------------------|--------------------|------------------------------------------------------------------------|-------------------------------------|\n",
        "| Q1                         |                    |                                                                        |                                     |\n",
        "| Q2                         |                    |                                                                        |                                     |\n",
        "| Q3                         |                    |                                                                        |                                     |\n",
        "| Q4                         |                    |                                                                        |                                     |\n",
        "\n",
        "---\n",
        "\n",
        "### Q3. **Did you refer to any other sources or websites apart from the LLM and lecture slides?**  \n",
        "\n",
        "If yes, please mention the source(s) clearly:\n",
        "> _Example: Research papers, blogs, documentation, textbooks, etc._\n",
        "\n",
        "---\n",
        "\n",
        "### Q4. **Apart from using any LLM, what was your own contribution while solving this assignment?**  \n",
        "\n",
        "Briefly describe:\n",
        "- Your understanding of the concepts  \n",
        "- The reasoning you applied  \n",
        "- Any implementation or analysis done independently  \n",
        "\n",
        "> _Your answer here_\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "q0IXoye3A8aH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "uU8pLu1NRQU_"
      },
      "source": [
        "# OpenAI Gym Environments\n",
        "<a id=\"Environment\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5khm9pQuRQVA"
      },
      "outputs": [],
      "source": [
        "# all imports go in here\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Random-Maze Environment**"
      ],
      "metadata": {
        "id": "d6IPiykO_1mY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In this assignment we will be exploring a variant of the Random Maze Environment (RME) that we have been looking in the lectures.**"
      ],
      "metadata": {
        "id": "mAOfVY6v0pe-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The environment is represented as a grid world in Figure 1a. Random maze environment\n",
        "is a highly stochastic environment with 11 states: two terminal states (a goal state (G) and a hole state (H))\n",
        "and 9 non-terminal states and a wall in between the environment. The wall behaves similar to the wall on the\n",
        "periphery of the environment, basically if an agent bumps against the wall, it bounces back. The boundary of\n",
        "the environment behaves similarly, if an agent hits the boundary it bounces back. The agent receives a reward\n",
        "of +1 when it lands in the goal state (3) and it receives a reward of -1 when it lands in the hole state (7).\n",
        "For rest of the transitions there is a reward of -0.04. Essentially the agent has the living cost of -0.04."
      ],
      "metadata": {
        "id": "ZR4uoVQmzto2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "O5iYWf4u0Div"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The transitions are stochastic as shown in Figure 1b. In this environment, four actions are possible: left, top, right,\n",
        "and bottom. For every intended action, there is 80% chance of going in the intended direction and remaining\n",
        "20% chances of going in either of the orthogonal directions. The 20% chance gets equally distributed between\n",
        "each of the orthogonal direction. The agent starts from state 8 (S). Assume γ = 0.99 for the problems below.\n"
      ],
      "metadata": {
        "id": "m7WTsT2qz2CP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "cjgaXT9l0PfQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment we will be looking at control algorithms we learnt in Lectures. For each of the plot, create\n",
        "the legend on the left/right side so that it doesn’t overlay on the plot. For all the algorithms below,\n",
        "this time we will not be specifying the hyper-parameters, please play with the hyper-params to come up with\n",
        "the best values. This way you will learn to tune the model. As you are aware from your past experience, single\n",
        "run of the algorithm over the environment results in plots that have lot of variance and look very noisy. One\n",
        "way to overcome this is to create several different instances of the environment using different seeds and then\n",
        "average out the results across these and plot these. For all the plots below, use this strategy.\n"
      ],
      "metadata": {
        "id": "hkauzbvjovtl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9LYjtzv-515"
      },
      "source": [
        "# Create the Maze Environment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SlipperyMaze(Env):\n",
        "    \"\"\"\n",
        "    A stochastic grid-based maze environment where actions may result\n",
        "    in unintended movements due to slippery transitions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config=None):\n",
        "        \"\"\"\n",
        "        Initializes the Slippery Maze Environment.\n",
        "\n",
        "        Args:\n",
        "            config (dict or None):\n",
        "                A configuration dictionary containing environment parameters.\n",
        "                It includes maze dimensions, number of states and actions,\n",
        "                start, goal, wall, and hole states, reward values, and\n",
        "                random seed information.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def _make_environment(self, config):\n",
        "        \"\"\"\n",
        "        Creates the transition probability model for the maze.\n",
        "\n",
        "        Args:\n",
        "            config (dict):\n",
        "                Dictionary defining the maze layout, total states,\n",
        "                total actions, maze shape, wall locations, and reward\n",
        "                specifications.\n",
        "\n",
        "        Returns:\n",
        "            dict:\n",
        "                A transition dictionary P such that:\n",
        "                P[state][action] is a list of tuples\n",
        "                (probability, next_state, reward, terminated),\n",
        "                describing the stochastic outcome of taking an action\n",
        "                in a given state.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def _is_terminated(self, state):\n",
        "        \"\"\"\n",
        "        Checks whether a given state is a terminal state.\n",
        "\n",
        "        Args:\n",
        "            state (int):\n",
        "                The current state index of the agent.\n",
        "\n",
        "        Returns:\n",
        "            bool:\n",
        "                True if the state corresponds to a goal or hole state,\n",
        "                otherwise False.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def _get_reward(self, state):\n",
        "        \"\"\"\n",
        "        Computes the reward associated with a given state.\n",
        "\n",
        "        Args:\n",
        "            state (int):\n",
        "                The state index reached after taking an action.\n",
        "\n",
        "        Returns:\n",
        "            float:\n",
        "                Reward value for the given state. This may represent\n",
        "                a goal reward, hole penalty, or living reward.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def _get_obs(self):\n",
        "        \"\"\"\n",
        "        Generates the observation returned to the agent.\n",
        "\n",
        "        Returns:\n",
        "            dict:\n",
        "                A dictionary containing:\n",
        "                - agent: the agent's current state\n",
        "                - terminals: list of terminal states\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def _get_info(self):\n",
        "        \"\"\"\n",
        "        Provides auxiliary diagnostic information.\n",
        "\n",
        "        Returns:\n",
        "            dict:\n",
        "                A dictionary containing additional information such as\n",
        "                the Manhattan distance between the agent and the goal state.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"\n",
        "        Resets the environment to its initial configuration.\n",
        "\n",
        "        Args:\n",
        "            seed (int or None):\n",
        "                Random seed used to ensure reproducibility.\n",
        "            options (dict or None):\n",
        "                Additional reset options (if any).\n",
        "\n",
        "        Returns:\n",
        "            observation (dict):\n",
        "                Initial observation of the environment.\n",
        "            info (dict):\n",
        "                Additional information after reset.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Executes one time step within the environment.\n",
        "\n",
        "        Args:\n",
        "            action (int):\n",
        "                Action selected by the agent. Represents a movement\n",
        "                direction in the maze.\n",
        "\n",
        "        Returns:\n",
        "            observation (dict):\n",
        "                Observation after performing the action.\n",
        "            reward (float):\n",
        "                Reward received after the action.\n",
        "            terminated (bool):\n",
        "                Whether the episode has reached a terminal state.\n",
        "            truncated (bool):\n",
        "                Whether the episode was truncated due to time limits.\n",
        "            info (dict):\n",
        "                Additional diagnostic information including transition logs.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"\n",
        "        Renders the current state of the environment.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def _render_frame(self):\n",
        "        \"\"\"\n",
        "        Renders a single frame of the environment for visualization.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"\n",
        "        Cleans up environment resources.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        pass"
      ],
      "metadata": {
        "id": "Et_P4GIF4E0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the environment\n",
        "# Register your custom environment with Gym/Gymnasium here\n",
        "# Use a unique environment ID and your environment class as the entry point\n",
        "# This registration will allow gym.make() to create the environment\n"
      ],
      "metadata": {
        "id": "3o8whEAmLGwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIG = {\n",
        "    'seed': _,\n",
        "    'environment': {\n",
        "        'total_states': _,\n",
        "        'maze_shape': (_, _),\n",
        "        'living_reward': _,\n",
        "        'total_actions': _,\n",
        "        'start_state': _,\n",
        "        'goal_state': _,\n",
        "        'hole_state': _,\n",
        "        'goal_reward': _,\n",
        "        'hole_reward': _,\n",
        "        'wall_state': _,\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "vtMx2Mm5LORc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define some utility function: You are free to use or not use them & define yours"
      ],
      "metadata": {
        "id": "8YzvVWJlS9Sj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate Trajectory based on Policy $\\pi$ and return experience tuples $(s, a, r, s^{\\prime})$"
      ],
      "metadata": {
        "id": "P74froRxLTRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_episode_trajectory(environment, Q, epsilon=None, config=None):\n",
        "    \"\"\"\n",
        "    Generates a single episode trajectory by interacting with the environment\n",
        "    using an epsilon-greedy policy derived from a given Q-function.\n",
        "\n",
        "    Args:\n",
        "        environment (gym.Env):\n",
        "            The environment in which the agent operates. It must support\n",
        "            reset() and step() methods and define a discrete action space.\n",
        "\n",
        "        Q (numpy.ndarray):\n",
        "            The action-value function (Q-table) of shape\n",
        "            (number_of_states, number_of_actions). It is used to select\n",
        "            greedy actions during exploitation.\n",
        "\n",
        "        epsilon (float or None):\n",
        "            Probability of selecting a random action instead of the greedy\n",
        "            action. If None, the value is taken from the configuration.\n",
        "\n",
        "        config (dict):\n",
        "            Configuration dictionary containing episode-level parameters.\n",
        "            This includes:\n",
        "            - random seed\n",
        "            - exploration rate\n",
        "            - maximum number of steps per episode\n",
        "            - environment-specific parameters\n",
        "\n",
        "    Returns:\n",
        "        list:\n",
        "            A list of tuples representing the episode trajectory.\n",
        "            Each tuple has the form:\n",
        "            (current_state, action, reward, next_state)\n",
        "\n",
        "            If the episode exceeds the maximum allowed number of steps,\n",
        "            an empty list is returned.\n",
        "    \"\"\"\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "Q2OoR5NELfMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIG = {\n",
        "    'seed': _,\n",
        "    'max_episodes': _,\n",
        "    'discount_factor': _,\n",
        "    'policy_name': '_',\n",
        "    'policy_action': _,\n",
        "    'max_steps': _,\n",
        "    'check_trajectories': _,\n",
        "    'epsilon': 0._, # _0% chance of exploration\n",
        "    'environment': {\n",
        "        'total_states': _,\n",
        "        'maze_shape': (_, _),\n",
        "        'living_reward': _,\n",
        "        'total_actions': _,\n",
        "        'start_state': _,\n",
        "        'goal_state': _,\n",
        "        'hole_state': _,\n",
        "        'goal_reward': _,\n",
        "        'hole_reward': _,\n",
        "        'wall_state': _,\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "QosZCoQOLrfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UN2DURN-51_"
      },
      "source": [
        "### Decay $\\alpha$"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decay_step_size(initial_value, final_value, episode, max_episode, decay_type, decay_stop=None):\n",
        "    \"\"\"\n",
        "    Computes the step size (learning rate) for a given episode based on\n",
        "    a specified decay strategy.\n",
        "\n",
        "    Args:\n",
        "        initial_value (float):\n",
        "            Initial step size at the beginning of training.\n",
        "\n",
        "        final_value (float):\n",
        "            Final step size to which the learning rate decays.\n",
        "\n",
        "        episode (int):\n",
        "            Current episode index for which the step size is computed.\n",
        "\n",
        "        max_episode (int):\n",
        "            Total number of episodes over which decay is applied.\n",
        "\n",
        "        decay_type (str):\n",
        "            Type of decay strategy used for step size scheduling.\n",
        "            Supported values include 'exponential' and 'linear'.\n",
        "\n",
        "        decay_stop (int or None):\n",
        "            Optional episode index after which the step size remains\n",
        "            constant at the final value.\n",
        "\n",
        "    Returns:\n",
        "        float:\n",
        "            Step size corresponding to the given episode based on the\n",
        "            chosen decay strategy.\n",
        "    \"\"\"\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "Ki5gOwWQMcnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_episode_step_sizes(initial_value, final_value, max_episode, decay_type, decay_stop=None):\n",
        "    \"\"\"\n",
        "    Generates step sizes for all episodes using a specified decay strategy.\n",
        "\n",
        "    Args:\n",
        "        initial_value (float):\n",
        "            Initial step size at the start of training.\n",
        "\n",
        "        final_value (float):\n",
        "            Final step size at the end of decay.\n",
        "\n",
        "        max_episode (int):\n",
        "            Total number of episodes for which step sizes are generated.\n",
        "\n",
        "        decay_type (str):\n",
        "            Decay strategy used to compute step sizes\n",
        "            (e.g., 'exponential' or 'linear').\n",
        "\n",
        "        decay_stop (int or None):\n",
        "            Optional episode index after which the step size\n",
        "            remains fixed.\n",
        "\n",
        "    Returns:\n",
        "        list:\n",
        "            A list of step size values, where each entry corresponds\n",
        "            to the step size for a particular episode.\n",
        "    \"\"\"\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "4dNONaKWMpy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_step_size_decay(step_sizes, max_steps, decay_type):\n",
        "    \"\"\"\n",
        "    Visualizes the decay of step size across episodes.\n",
        "\n",
        "    Args:\n",
        "        step_sizes (list or numpy.ndarray):\n",
        "            Sequence of step size values for each episode.\n",
        "\n",
        "        max_steps (int):\n",
        "            Number of episodes over which the step sizes are plotted.\n",
        "\n",
        "        decay_type (str):\n",
        "            Name of the decay strategy used, displayed in the plot title.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "a7cYXs1cMs6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIG = {\n",
        "    \"seed\": _,\n",
        "    'max_episodes': _,\n",
        "    'discount_factor': _,\n",
        "    'policy_name': '_',\n",
        "    'policy_action': _,\n",
        "    'max_steps': _,\n",
        "    'environment': {\n",
        "        'total_states': _,\n",
        "        'maze_shape': (_, _),\n",
        "        'living_reward': _,\n",
        "        'total_actions': _,\n",
        "        'start_state': _,\n",
        "        'goal_state': _,\n",
        "        'hole_state': _,\n",
        "        'goal_reward': _,\n",
        "        'hole_reward': _,\n",
        "        'wall_state': _,\n",
        "    },\n",
        "    'step_size': {\n",
        "        'initial_value': _,\n",
        "        'final_value': _,\n",
        "        'decay_type': '_',\n",
        "        'decay_stop': _\n",
        "    }\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "6mVVSQTpMwoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAI4XgmM-52A"
      },
      "source": [
        "### Decay $\\epsilon$"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decay_epsilon(initial_value, final_value, episode, max_episode, decay_type, decay_stop=None):\n",
        "    \"\"\"\n",
        "    Computes the exploration rate (epsilon) for a given episode based on\n",
        "    a specified decay strategy.\n",
        "\n",
        "    Args:\n",
        "        initial_value (float):\n",
        "            Initial value of epsilon at the beginning of training.\n",
        "\n",
        "        final_value (float):\n",
        "            Minimum value of epsilon after decay.\n",
        "\n",
        "        episode (int):\n",
        "            Current episode index for which epsilon is computed.\n",
        "\n",
        "        max_episode (int):\n",
        "            Total number of episodes over which epsilon decay is applied.\n",
        "\n",
        "        decay_type (str):\n",
        "            Type of decay strategy used for epsilon scheduling.\n",
        "            Supported values include 'exponential' and 'linear'.\n",
        "\n",
        "        decay_stop (int or None):\n",
        "            Optional episode index after which epsilon remains\n",
        "            fixed at the final value.\n",
        "\n",
        "    Returns:\n",
        "        float:\n",
        "            Epsilon value corresponding to the given episode based on\n",
        "            the chosen decay strategy.\n",
        "    \"\"\"\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "CkesULu7M6yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_episode_epsilons(initial_value, final_value, max_episode, decay_type, decay_stop=None):\n",
        "    \"\"\"\n",
        "    Generates epsilon values for all episodes using a specified decay strategy.\n",
        "\n",
        "    Args:\n",
        "        initial_value (float):\n",
        "            Initial epsilon value at the start of training.\n",
        "\n",
        "        final_value (float):\n",
        "            Final epsilon value after decay.\n",
        "\n",
        "        max_episode (int):\n",
        "            Total number of episodes for which epsilon values are generated.\n",
        "\n",
        "        decay_type (str):\n",
        "            Decay strategy used for computing epsilon values\n",
        "            (e.g., 'linear' or 'exponential').\n",
        "\n",
        "        decay_stop (int or None):\n",
        "            Optional episode index after which epsilon remains constant.\n",
        "\n",
        "    Returns:\n",
        "        list:\n",
        "            A list of epsilon values, where each element corresponds\n",
        "            to the exploration rate for a particular episode.\n",
        "    \"\"\"\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "BK28FJZtM8p8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_epsilon_decay(step_sizes, max_steps, decay_type):\n",
        "    \"\"\"\n",
        "    Visualizes the decay of epsilon across training episodes.\n",
        "\n",
        "    Args:\n",
        "        step_sizes (list or numpy.ndarray):\n",
        "            Sequence of epsilon values across episodes.\n",
        "\n",
        "        max_steps (int):\n",
        "            Number of episodes over which epsilon decay is plotted.\n",
        "\n",
        "        decay_type (str):\n",
        "            Name of the decay strategy used, displayed in the plot title.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "3vIhGEgzNBM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIG = {\n",
        "    \"seed\": _,\n",
        "    'max_episodes': _,\n",
        "    'discount_factor': _,\n",
        "    'policy_name': '_',\n",
        "    'policy_action': _,\n",
        "    'max_steps': _,\n",
        "    'environment': {\n",
        "        'total_states': _,\n",
        "        'maze_shape': (_, _),\n",
        "        'living_reward': _,\n",
        "        'total_actions': _,\n",
        "        'start_state': _,\n",
        "        'goal_state': _,\n",
        "        'hole_state': _,\n",
        "        'goal_reward': _,\n",
        "        'hole_reward': _,\n",
        "        'wall_state': _,\n",
        "    },\n",
        "    'step_size': {\n",
        "        'initial_value': _,\n",
        "        'final_value': _,\n",
        "        'decay_type': '_',\n",
        "        'decay_stop': _\n",
        "    },\n",
        "    'epsilon': {\n",
        "        'initial_value': _,\n",
        "        'final_value': _,\n",
        "        'decay_type': '_',\n",
        "        'decay_stop': _\n",
        "    }\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "iip_4reZND4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_policy_success(environment, policy, config=None):\n",
        "    \"\"\"\n",
        "    Evaluates whether a given policy successfully reaches the goal state\n",
        "    within a specified number of steps and computes the discounted return.\n",
        "\n",
        "    Args:\n",
        "        environment (gym.Env):\n",
        "            The environment in which the policy is evaluated. It must\n",
        "            support reset() and step() operations.\n",
        "\n",
        "        policy (list or numpy.ndarray):\n",
        "            A deterministic policy mapping each state to an action.\n",
        "            The index represents the state and the value represents\n",
        "            the action selected in that state.\n",
        "\n",
        "        config (dict):\n",
        "            Configuration dictionary containing evaluation parameters.\n",
        "            This includes:\n",
        "            - random seed\n",
        "            - maximum number of steps\n",
        "            - discount factor\n",
        "            - environment-specific goal state information\n",
        "\n",
        "    Returns:\n",
        "        success (bool):\n",
        "            True if the policy reaches the goal state within the allowed\n",
        "            number of steps, otherwise False.\n",
        "\n",
        "        return_reward (float):\n",
        "            Discounted cumulative reward obtained while following\n",
        "            the policy during the episode.\n",
        "    \"\"\"\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "xRv2wyBoNZxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_episode_values(V_r, image_name, config=None):\n",
        "    \"\"\"\n",
        "    Visualizes the evolution of value estimates for non-terminal states\n",
        "    across multiple episodes.\n",
        "\n",
        "    Args:\n",
        "        V_r (numpy.ndarray):\n",
        "            A 2D array containing value estimates for each state\n",
        "            across episodes. Each row corresponds to an episode,\n",
        "            and each column corresponds to a state.\n",
        "\n",
        "        image_name (str):\n",
        "            Name of the output image file in which the plot\n",
        "            will be saved.\n",
        "\n",
        "        config (dict):\n",
        "            Configuration dictionary containing experiment parameters.\n",
        "            This includes:\n",
        "            - total number of episodes\n",
        "            - environment-specific state information\n",
        "            - plotting-related settings\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "qNtnG2neNhWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actions = ['right', 'up', 'left', 'down']\n"
      ],
      "metadata": {
        "id": "w2o6SSgNNoq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_episode_q_values(Q_r, image_name, config=None):\n",
        "    \"\"\"\n",
        "    Visualizes the evolution of action-value (Q) estimates for all actions\n",
        "    and non-terminal states across multiple episodes.\n",
        "\n",
        "    Args:\n",
        "        Q_r (numpy.ndarray):\n",
        "            A 3D array containing Q-value estimates.\n",
        "            Dimensions correspond to:\n",
        "            - episodes\n",
        "            - states\n",
        "            - actions\n",
        "\n",
        "        image_name (str):\n",
        "            Name of the output image file in which the plot\n",
        "            will be saved.\n",
        "\n",
        "        config (dict):\n",
        "            Configuration dictionary containing experiment parameters.\n",
        "            This includes:\n",
        "            - number of episodes\n",
        "            - total number of states\n",
        "            - total number of actions\n",
        "            - environment-specific terminal and wall states\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "Gjm2a2qWNrBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_tests(n_tests=50, method=None, config=None):\n",
        "    \"\"\"\n",
        "    Executes multiple independent experimental trials for a given\n",
        "    reinforcement learning method and aggregates the results.\n",
        "\n",
        "    Args:\n",
        "        n_tests (int):\n",
        "            Number of independent test runs to be performed. Each test\n",
        "            uses a different random seed to evaluate robustness and\n",
        "            stability of the learning method.\n",
        "\n",
        "        method (callable):\n",
        "            A reinforcement learning control method to be evaluated.\n",
        "            This function is expected to take an environment and a\n",
        "            configuration dictionary as input and return learned\n",
        "            value functions, policies, and performance metrics.\n",
        "\n",
        "        config (dict):\n",
        "            Configuration dictionary containing experiment and\n",
        "            environment parameters. This includes:\n",
        "            - number of episodes\n",
        "            - environment settings\n",
        "            - learning hyperparameters\n",
        "            - random seed information\n",
        "\n",
        "    Returns:\n",
        "        Qs (numpy.ndarray):\n",
        "            A 4D array containing Q-value estimates for all tests,\n",
        "            episodes, states, and actions.\n",
        "\n",
        "        V_e (numpy.ndarray):\n",
        "            A 3D array containing state-value estimates for all tests,\n",
        "            episodes, and states.\n",
        "\n",
        "        policy_success (numpy.ndarray):\n",
        "            A 2D boolean array indicating whether the learned policy\n",
        "            successfully reached the goal state in each episode\n",
        "            across all tests.\n",
        "\n",
        "        return_reward (numpy.ndarray):\n",
        "            A 2D array containing the discounted return obtained\n",
        "            in each episode across all test runs.\n",
        "    \"\"\"\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "HZlCgChoNyzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzaFJ3BS-52C"
      },
      "source": [
        "# Initialize parameters which can be useful later (Optional Here)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5Yt1tU2-52C"
      },
      "outputs": [],
      "source": [
        "policy_success = np.zeros((4, 50, CONFIG['max_episodes']), dtype=bool)\n",
        "return_reward = np.zeros((4, 50, CONFIG['max_episodes']))\n",
        "state_value_estimation = np.zeros((4, CONFIG['max_episodes'], CONFIG['environment']['total_states']))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 1: Monte Carlo Control (40 points)\n",
        "($40+20+20+5+5+5+5=100$ points)\n",
        "\n",
        "Implement the Monte Carlo Control for the Random Maze Environment (RME) described above. In particular, you need to implement First Visit Monte Carlo Control (FMVCC) for finding the optimal policy for\n",
        "RME. Use the function definition (given below) as given in Lecture slides.\n",
        "MonteCarloControl(env, $\\gamma$, $\\alpha_0$, $\\epsilon_0$, maxSteps, noEpisodes, firstVisit = True)\n"
      ],
      "metadata": {
        "id": "jqSMG3xUxV2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def monte_carlo_control(environment, config=None):\n",
        "   \"\"\"\n",
        "    Implements Monte Carlo Control for a discrete Random Maze Environment (RME).\n",
        "\n",
        "    This function applies First-Visit Monte Carlo Control (FMVCC) to learn the\n",
        "    optimal action-value function Q(s, a) and the corresponding optimal policy.\n",
        "    An epsilon-greedy policy is used for exploration, with optional decay of\n",
        "    epsilon and step size across episodes.\n",
        "\n",
        "    Args:\n",
        "        environment (gym.Env): Environment with discrete observation and action spaces.\n",
        "        config (dict): Configuration dictionary containing:\n",
        "            - max_episodes (int): Number of Monte Carlo episodes.\n",
        "            - discount_factor (float): Discount factor γ.\n",
        "            - step_size (dict): Step-size (alpha) decay parameters.\n",
        "            - epsilon (dict): Epsilon-greedy decay parameters.\n",
        "            - monte_carlo_strategy (str): 'first_visit' or 'every_visit'.\n",
        "\n",
        "    Returns:\n",
        "        Q (np.ndarray): Final learned action-value function of shape (S, A).\n",
        "        V_e (np.ndarray): State-value estimates per episode.\n",
        "        policy (np.ndarray): Final greedy policy derived from Q.\n",
        "        Qs (np.ndarray): History of Q-values across episodes.\n",
        "        mc_target_tensor (np.ndarray): Monte Carlo return targets for each (s, a).\n",
        "        policy_success (np.ndarray): Boolean success indicator per episode.\n",
        "        return_reward (np.ndarray): Total return obtained per episode.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "   pass"
      ],
      "metadata": {
        "id": "XdhNFPqoz0UH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (a) Plot State-value (20 points)\n",
        "Plot evolution of State-value (V) function with time. Basically, plot V-function vs Episodes for states 0,\n",
        "1, 2, 4, 6, 8, 9, 10, 11 in a single plot. Also plot the true V-function for each of the state in the same plot.\n"
      ],
      "metadata": {
        "id": "YRzY8C0HyGmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_episode_values ()"
      ],
      "metadata": {
        "id": "BDI9k67R1xg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(b) Plot Action state value (20 points)\n",
        "Plot evolution of action state value (Q) function with time. Plot Q function vs Episodes for states 0, 1,\n",
        "2, 4, 6, 8, 9, 10, 11 in a single plot. Also plot the true Q-function for each of the state in the same plot.\n"
      ],
      "metadata": {
        "id": "LnPeDaszyKis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_episode_q_values ()"
      ],
      "metadata": {
        "id": "oSe0XEMx11do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(c) Instances and seeds (5 Points)\n",
        " Describe over how many instances of the environments did you average the results? Write about the seeds used for each instance.\n"
      ],
      "metadata": {
        "id": "k9EPc3ZtyOp-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "TEOLbIBzAXeh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(d) Environment diagram (5 points)\n",
        "Draw the environment diagram with optimal policy (shown using arrows) obtained using the algorithm.\n"
      ],
      "metadata": {
        "id": "kXLLLPPDyUFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "b9Ub0IzXAdqy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(e) Hyper-parameters (5 points)\n",
        "Write about the hyper-parameters you finally used for the algorithm and describe how did you arrive at these set of hyper-params."
      ],
      "metadata": {
        "id": "YTarMOMTyY4B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "nqQ9LYI0AgBa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(f) Observations (5 points)\n",
        "Write about your observations from the plots above."
      ],
      "metadata": {
        "id": "30mkGdKTycNF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "TPRSrglcAlbd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 2: SARSA (TD Control)\n",
        "($40+20+20+5+5+5+5=100$ points)\n",
        "\n",
        "Implement the SARSA algorithm for the Random Maze Environment (RME) described above. Use the function definition as given in Lecture slides.\n",
        "SARSA(env,$\\gamma$, $\\alpha_0$, $\\epsilon_0$, noEpisodes)(40 Points)\n",
        "\n"
      ],
      "metadata": {
        "id": "t1QJPC132PV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sarsa_control(environment, config=None):\n",
        "    \"\"\"\n",
        "    Implements the SARSA (State–Action–Reward–State–Action) algorithm for a\n",
        "    discrete Random Maze Environment (RME).\n",
        "\n",
        "    This function applies the on-policy Temporal Difference (TD) control method\n",
        "    SARSA to learn the optimal action-value function Q(s, a). An epsilon-greedy\n",
        "    policy is used for both action selection and policy evaluation, with optional\n",
        "    decay of the learning rate (alpha) and exploration rate (epsilon) across\n",
        "    episodes.\n",
        "\n",
        "    Args:\n",
        "        environment (gym.Env): Environment with discrete observation and action spaces.\n",
        "        config (dict): Configuration dictionary containing:\n",
        "            - max_episodes (int): Number of training episodes.\n",
        "            - discount_factor (float): Discount factor γ.\n",
        "            - step_size (dict): Learning rate (alpha) decay parameters.\n",
        "            - epsilon (dict): Epsilon-greedy exploration decay parameters.\n",
        "            - seed (int): Random seed for environment reset.\n",
        "\n",
        "    Returns:\n",
        "        Q (np.ndarray): Final learned action-value function of shape (S, A).\n",
        "        V_s (np.ndarray): State-value estimates derived from Q for each episode.\n",
        "        policy (np.ndarray): Final greedy policy derived from Q.\n",
        "        Qs (np.ndarray): History of Q-values across episodes.\n",
        "        td_target (float): Last computed TD target value.\n",
        "        policy_success (np.ndarray): Boolean success indicator per episode.\n",
        "        return_reward (np.ndarray): Total return obtained per episode.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "3dGq-fela7Dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(a) State-value function (20 Points)\n",
        "Plot evolution of State-value (V) function with time. Basically, plot V-function vs Episodes for states 0,\n",
        "1, 2, 4, 6, 8, 9, 10, 11 in a single plot. Also plot the true V-function for each of the state in the same plot.\n"
      ],
      "metadata": {
        "id": "kHp3V34E45bd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_episode_values ()"
      ],
      "metadata": {
        "id": "PLI-bCoxb1A6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(b) Action state value (Q) function (20 Points)\n",
        "Plot evolution of action state value (Q) function with time. Plot Q function vs Episodes for states 0, 1,\n",
        "2, 4, 6, 8, 9, 10, 11 in a single plot. Also plot the true Q-function for each of the state in the same plot.\n"
      ],
      "metadata": {
        "id": "hOuxAzPg49fs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_episode_q_values ()"
      ],
      "metadata": {
        "id": "ztWq7WQXb2YX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(c) Instances and seeds (5 Points)\n",
        "Describe over how many instances of the environments did you average the results? Write about the seeds used for each instance.\n"
      ],
      "metadata": {
        "id": "4C_7U89p5AYY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "GQa6JACUFhYE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(d) Environment diagram (5 Points)\n",
        "Draw the environment diagram with optimal policy (shown using arrows) obtained using the algorithm.\n"
      ],
      "metadata": {
        "id": "xeL6K67o5D10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "ud3knE09FiZT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(e) Hyper-parameters (5 Points)\n",
        "Write about the hyper-parameters you finally used for the algorithm and describe how did you arrive at these set of hyper-params.\n"
      ],
      "metadata": {
        "id": "r_yLvu6x5HJb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "qHz5LQDeFmoo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(f) Observations (5 Points)\n",
        "Write about your observations from the plots above."
      ],
      "metadata": {
        "id": "cmlLMK9F5KPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "e_4_X61BFoCu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 3: Q-Learning\n",
        "($40+20+20+5+5+5+5=100$ points)\n",
        "\n",
        "Implement the Q-Learning algorithm for the Random Maze Environment (RME) described above. Use the function definition as given in Lecture slides.\n",
        "Q-Learning(env, $\\gamma$, $\\alpha_0$, $\\epsilon_0$, noEpisodes) (40 Points)"
      ],
      "metadata": {
        "id": "wjkF1vYB7hCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def q_learning_control(environment, config=None):\n",
        "    \"\"\"\n",
        "    Implements the Q-Learning algorithm for a discrete Random Maze Environment (RME).\n",
        "\n",
        "    This function applies Q-Learning, an off-policy Temporal Difference (TD) control\n",
        "    algorithm, to learn the optimal action-value function Q(s, a). Action selection\n",
        "    is performed using an epsilon-greedy behavior policy, while updates are based on\n",
        "    the greedy target policy. Learning rate (alpha) and exploration rate (epsilon)\n",
        "    may decay across episodes.\n",
        "\n",
        "    Args:\n",
        "        environment (gym.Env): Environment with discrete observation and action spaces.\n",
        "        config (dict): Configuration dictionary containing:\n",
        "            - max_episodes (int): Number of training episodes.\n",
        "            - discount_factor (float): Discount factor γ.\n",
        "            - step_size (dict): Learning rate (alpha) decay parameters.\n",
        "            - epsilon (dict): Epsilon-greedy exploration decay parameters.\n",
        "            - seed (int): Random seed for environment reset.\n",
        "\n",
        "    Returns:\n",
        "        Q (np.ndarray): Final learned action-value function of shape (S, A).\n",
        "        V_s (np.ndarray): State-value estimates derived from Q for each episode.\n",
        "        policy (np.ndarray): Final greedy policy derived from Q.\n",
        "        Qs (np.ndarray): History of Q-values across episodes.\n",
        "        None: Placeholder (no TD target history stored).\n",
        "        policy_success (np.ndarray): Boolean success indicator per episode.\n",
        "        return_reward (np.ndarray): Total return obtained per episode.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "DlVc8-1xdAUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(a) State-value (V) function (20 Points)\n",
        "Plot evolution of State-value (V) function with time. Basically, plot V-function vs Episodes for states 0,\n",
        "1, 2, 4, 6, 8, 9, 10, 11 in a single plot. Also plot the true V-function for each of the state in the same plot."
      ],
      "metadata": {
        "id": "xyH2bBBr-TIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_episode_values ()"
      ],
      "metadata": {
        "id": "89EkbmUedIci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(b)Action state value (Q) function (20 Points)\n",
        "Plot evolution of action state value (Q) function with time. Plot Q function vs Episodes for states 0, 1,\n",
        "2, 4, 6, 8, 9, 10, 11 in a single plot. Also plot the true Q-function for each of the state in the same plot."
      ],
      "metadata": {
        "id": "ZtWE1pHx-gPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_episode_q_values ()"
      ],
      "metadata": {
        "id": "cSZhKFAodMXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(c) Instances and seeds (5 Points)\n",
        "Describe over how many instances of the environments did you average the results? Write about the seeds used for each instance."
      ],
      "metadata": {
        "id": "y_LPDm_m-k1Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "ETAbH0rEFpyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(d) Environment diagram (5 Points)\n",
        "Draw the environment diagram with optimal policy (shown using arrows) obtained using the algorithm."
      ],
      "metadata": {
        "id": "B7UImzNQ-oZm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "FvlxR2W4Fqug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(e) hyper-parameters (5 Points)\n",
        "Write about the hyper-parameters you finally used for the algorithm and describe how did you arrive at\n",
        "these set of hyper-params."
      ],
      "metadata": {
        "id": "WS1DMoFb-tdD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "Ebcp2SGaFsGG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(f) Observations (5 Points)\n",
        "Write about your observations from the plots above."
      ],
      "metadata": {
        "id": "y5S6Zbr8-wTY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "f6fgFxgrFtB4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem 4: Double Q-Learning**\n",
        "\n",
        "($40+20+20+5+5+5+5=100$ points)\n",
        "\n",
        "Implement the Double Q-Learning algorithm for the Random Maze Environment (RME) described above.\n",
        "Use the function definition as given in Lecture slides.\n",
        "\n",
        "Double-Q-Learning(env, $\\gamma$, $\\alpha_0$, $\\epsilon_0$, noEpisodes) (40 Points)"
      ],
      "metadata": {
        "id": "GFxzaKXNDAhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def double_q_learning_control(environment, config=None):\n",
        "    \"\"\"\n",
        "    Implements the Double Q-Learning algorithm for a discrete Random Maze\n",
        "    Environment (RME).\n",
        "\n",
        "    This function applies Double Q-Learning, an off-policy Temporal Difference\n",
        "    (TD) control algorithm designed to reduce maximization bias present in\n",
        "    standard Q-Learning. Two independent action-value functions (Q1 and Q2)\n",
        "    are learned by alternating updates, while action selection is performed\n",
        "    using an epsilon-greedy policy derived from their average.\n",
        "\n",
        "    Args:\n",
        "        environment (gym.Env): Environment with discrete observation and action spaces.\n",
        "        config (dict): Configuration dictionary containing:\n",
        "            - max_episodes (int): Number of training episodes.\n",
        "            - discount_factor (float): Discount factor γ.\n",
        "            - step_size (dict): Learning rate (alpha) decay parameters.\n",
        "            - epsilon (dict): Epsilon-greedy exploration decay parameters.\n",
        "            - seed (int): Random seed for environment reset.\n",
        "\n",
        "    Returns:\n",
        "        Q (np.ndarray): Final combined action-value function (average of Q1 and Q2).\n",
        "        V_s (np.ndarray): State-value estimates derived from Q for each episode.\n",
        "        policy (np.ndarray): Final greedy policy derived from Q.\n",
        "        Qs (np.ndarray): History of combined Q-values across episodes.\n",
        "        None: Placeholder (no TD target history stored).\n",
        "        policy_success (np.ndarray): Boolean success indicator per episode.\n",
        "        return_reward (np.ndarray): Total return obtained per episode.\n",
        "    \"\"\"\n",
        "\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "WGQd0ztYmLip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(a) State-value (V) function (20 Points)\n",
        "Plot evolution of State-value (V) function with time. Basically, plot V-function vs Episodes for states 0,\n",
        "1, 2, 4, 6, 8, 9, 10, 11 in a single plot. Also plot the true V-function for each of the state in the same plot."
      ],
      "metadata": {
        "id": "eKb4O3MLDj-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_episode_values ()"
      ],
      "metadata": {
        "id": "ZOEoItaYmc4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###(b) Action state value (Q) function (20 Points)\n",
        "Plot evolution of action state value (Q) function with time. Plot Q function vs Episodes for states 0, 1,\n",
        "2, 4, 6, 8, 9, 10, 11 in a single plot. Also plot the true Q-function for each of the state in the same plot.\n"
      ],
      "metadata": {
        "id": "7HeeTTHWDAeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_episode_q_values ()"
      ],
      "metadata": {
        "id": "pTWPRj-umeou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(c) Instances and seeds (5 Points)\n",
        "Describe over how many instances of the environments did you average the results? Write about the seeds\n",
        "used for each instance.\n"
      ],
      "metadata": {
        "id": "g0F8O31rDAb7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "U4E4FIbwFvpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(d) Environment diagram (5 Points)\n",
        "Draw the environment diagram with optimal policy (shown using arrows) obtained using the algorithm.\n"
      ],
      "metadata": {
        "id": "GpMBlk9jDAZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "9rtozp97FwnW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(e) hyper-parameters (5 Points)\n",
        "Write about the hyper-parameters you finally used for the algorithm and describe how did you arrive at\n",
        "these set of hyper-params.\n"
      ],
      "metadata": {
        "id": "jZ1-XA5eDAXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "FF8OOq7BFyWs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(f) Observations (5 Points)\n",
        "Write about your observations from the plots above.\n"
      ],
      "metadata": {
        "id": "v6ElrWxFDALc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "wMzfc0U4Fz_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem 5: Comparing Control Algorithms**\n",
        "($20+5+5+5+5+5=40$ points)\n",
        "\n",
        "For FVMCC, SARSA, Q and Double-Q algorithms implemented above, do the following:"
      ],
      "metadata": {
        "id": "nEi33RL-CZEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(a)Policy Success Rate (in %) vs Episodes (20 Points)\n",
        "For each of the algorithm, in a single plot, plot the evolution of Policy Success Rate (in %) vs Episodes.\n",
        "Policy Success Rate is defined as number of times the agent reaches the goal state out of the total number\n",
        "of the episodes run using a specific policy. Basically implement the following function that would return\n",
        "the policy success percentage. As you are training the agent, at each episode, you will have a version of\n",
        "the policy, use that policy along with the function below to get the policy success rate.\n",
        "\n",
        "def getPolicySuccessRate(env, $π_\\text{current}$, goalState, maxEpisodes=100, maxSteps=200)"
      ],
      "metadata": {
        "id": "LYbMipKdCZBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_policy_success(policy_success, title, image_name, config=None):\n",
        "    \"\"\"\n",
        "    Plots the evolution of Policy Success Rate (%) versus Episodes for multiple\n",
        "    reinforcement learning algorithms.\n",
        "\n",
        "    This function computes the mean policy success rate across runs for each\n",
        "    algorithm and plots them on a single graph. Policy Success Rate is defined\n",
        "    as the percentage of episodes in which the agent reaches the goal state\n",
        "    when following the learned policy at that episode.\n",
        "\n",
        "    The plot includes curves for:\n",
        "        - First-Visit Monte Carlo Control (FVMC)\n",
        "        - SARSA\n",
        "        - Q-Learning\n",
        "        - Double Q-Learning\n",
        "\n",
        "    Args:\n",
        "        policy_success (np.ndarray): Boolean array indicating policy success\n",
        "            across episodes for each algorithm. Expected shape:\n",
        "            (num_algorithms, max_episodes, num_runs) or equivalent.\n",
        "        title (str): Title of the plot.\n",
        "        image_name (str): Filename for saving the generated plot.\n",
        "        config (dict): Configuration dictionary containing:\n",
        "            - max_episodes (int): Number of training episodes.\n",
        "\n",
        "    Returns:\n",
        "        None: The function saves the plot to disk.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "fSKSopyHojUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###(b)Policy Success Rate (in %) plot (5 Points)\n",
        "What are your observations from the Policy Success Rate (in %) plot.\n"
      ],
      "metadata": {
        "id": "PSqGA_FDCY-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[write your answer here]"
      ],
      "metadata": {
        "id": "flqxDuHqowFS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(c) Estimated Expected Return (5 Points)\n",
        "For each of the algorithm (in a single plot), plot the Estimated Expected Return (from the start state) vs\n",
        "Episodes.\n"
      ],
      "metadata": {
        "id": "baV8M1ZnCY8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_expected_return(return_reward, title, image_name, config=None):\n",
        "    \"\"\"\n",
        "    Plots the Estimated Expected Return from the start state versus Episodes\n",
        "    for multiple reinforcement learning algorithms.\n",
        "\n",
        "    This function computes the average expected return across runs for each\n",
        "    algorithm at every episode and visualizes their learning performance on\n",
        "    a single plot. The expected return reflects the cumulative discounted\n",
        "    reward obtained by following the learned policy from the start state.\n",
        "\n",
        "    The plot includes curves for:\n",
        "        - First-Visit Monte Carlo Control (FVMC)\n",
        "        - SARSA\n",
        "        - Q-Learning\n",
        "        - Double Q-Learning\n",
        "\n",
        "    Args:\n",
        "        return_reward (np.ndarray): Array containing episode returns for each\n",
        "            algorithm. Expected shape:\n",
        "            (num_algorithms, max_episodes, num_runs) or equivalent.\n",
        "        title (str): Title of the plot.\n",
        "        image_name (str): Filename for saving the generated plot.\n",
        "        config (dict): Configuration dictionary containing:\n",
        "            - max_episodes (int): Number of training episodes.\n",
        "\n",
        "    Returns:\n",
        "        None: The function saves the plot to disk.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "dCeka6bHo1Pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(d) observations (5 Points)\n",
        "What are your observations for the Estimated Expected Return plot?\n"
      ],
      "metadata": {
        "id": "XkVEtqR9CY5z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[write your answer here]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "soOIVZH8pIkK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(e)State-value Function Estimation Error vs Episodes (5 Points)\n",
        "\n",
        "For each of the algorithm (in a single plot), plot the State-value Function Estimation Error vs Episodes.\n",
        "State-value Function Estimation Error is defined as Mean Absolute Error across all V-function estimates\n",
        "(across all states) from the respective optimal value.\n"
      ],
      "metadata": {
        "id": "apuqihdVCY3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_state_value_error(state_values, title, image_name, config=None):\n",
        "    \"\"\"\n",
        "    Plots the State-Value Function Estimation Error versus Episodes for multiple\n",
        "    reinforcement learning algorithms.\n",
        "\n",
        "    The state-value function estimation error is defined as the Mean Absolute\n",
        "    Error (MAE) between the estimated state-value function V(s) and the true\n",
        "    optimal state-value function, averaged across all non-terminal states.\n",
        "    This metric evaluates how accurately each algorithm approximates the\n",
        "    optimal value function over training.\n",
        "\n",
        "    The plot includes curves for:\n",
        "        - First-Visit Monte Carlo Control (FVMC)\n",
        "        - SARSA\n",
        "        - Q-Learning\n",
        "        - Double Q-Learning\n",
        "\n",
        "    Args:\n",
        "        state_values (np.ndarray): Estimated state-value functions for each\n",
        "            algorithm across episodes. Expected shape:\n",
        "            (num_algorithms, max_episodes, num_states).\n",
        "        title (str): Title of the plot.\n",
        "        image_name (str): Filename for saving the generated plot.\n",
        "        config (dict): Configuration dictionary containing:\n",
        "            - max_episodes (int): Number of training episodes.\n",
        "\n",
        "    Returns:\n",
        "        None: The function saves the plot to disk.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "I2TskFNXpMDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(f)observations for the State-value Function Estimation Error plot (5 Points)\n",
        "What are your observations for the State-value Function Estimation Error plot? (5 Points)"
      ],
      "metadata": {
        "id": "5BNuvF-MCY0w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "[write your answer here]"
      ],
      "metadata": {
        "id": "HBT1HLdFpiFr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem 6: SARSA($\\lambda$) Replacing**\n",
        "(40+20+20+5+5+5+5=100 points)\n",
        "\n",
        "Implement the SARSA(λ) algorithm with Replacing Eligibility Traces for the Random Maze Environment\n",
        "(RME) described above. Use the function definition as given in Lecture slides.\n",
        "\n",
        "SARSA-Lambda(env, γ, α0, ϵ0, λ, noEpisodes, replaceTrace = True)\n",
        "(40 Points)"
      ],
      "metadata": {
        "id": "QRm857B9Bdv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sarsa_lambda_control(environment, config=None):\n",
        "    \"\"\"\n",
        "    Implements the SARSA(λ) algorithm with eligibility traces for a discrete\n",
        "    Random Maze Environment (RME).\n",
        "\n",
        "    This function applies SARSA(λ), an on-policy Temporal Difference (TD) control\n",
        "    algorithm that extends SARSA by incorporating eligibility traces to speed up\n",
        "    learning. Eligibility traces allow credit assignment to multiple previously\n",
        "    visited state-action pairs. Both accumulating and replacing traces are\n",
        "    supported, as specified in the configuration.\n",
        "\n",
        "    Args:\n",
        "        environment (gym.Env): Environment with discrete observation and action spaces.\n",
        "        config (dict): Configuration dictionary containing:\n",
        "            - max_episodes (int): Number of training episodes.\n",
        "            - discount_factor (float): Discount factor γ.\n",
        "            - lambda (float): Eligibility trace decay parameter λ.\n",
        "            - trace (str): Type of eligibility trace ('accumulating' or 'replacing').\n",
        "            - step_size (dict): Learning rate (alpha) decay parameters.\n",
        "            - epsilon (dict): Epsilon-greedy exploration decay parameters.\n",
        "            - seed (int): Random seed for environment reset.\n",
        "\n",
        "    Returns:\n",
        "        Q (np.ndarray): Final learned action-value function of shape (S, A).\n",
        "        V_s (np.ndarray): State-value estimates derived from Q for each episode.\n",
        "        policy (np.ndarray): Final greedy policy derived from Q.\n",
        "        Qs (np.ndarray): History of Q-values across episodes.\n",
        "        td_target (float): Last computed TD target value.\n",
        "        policy_success (np.ndarray): Boolean success indicator per episode.\n",
        "        return_reward (np.ndarray): Total return obtained per episode.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "dkNt6f0NrnJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(a)State-value (V) function (20 Points)\n",
        "Plot evolution of State-value (V) function with time. Basically, plot V-function vs Episodes for states 0,\n",
        "1, 2, 4, 6, 8, 9, 10, 11 in a single plot. Also plot the true V-function for each of the state in the same plot.\n"
      ],
      "metadata": {
        "id": "9nevK_sYBdqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_episode_values ()"
      ],
      "metadata": {
        "id": "MaaHr4HaruZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(b)Action state value (Q) function(20 Points)\n",
        "Plot evolution of action state value (Q) function with time. Plot Q function vs Episodes for states 0, 1,\n",
        "2, 4, 6, 8, 9, 10, 11 in a single plot. Also plot the true Q-function for each of the state in the same plot.\n",
        "(20 Points)"
      ],
      "metadata": {
        "id": "ZQ1dKPbfBdnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_episode_q_values ()"
      ],
      "metadata": {
        "id": "-QPuOT85rvd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(c) Instances and seeds (5 Points)\n",
        "Describe over how many instances of the environments did you average the results? Write about the seeds\n",
        "used for each instance.\n"
      ],
      "metadata": {
        "id": "8fGPB4jjBdlO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[write your answer here]"
      ],
      "metadata": {
        "id": "F-ntju6xx0A8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(d) Environment diagram (5 Points)\n",
        "Draw the environment diagram with optimal policy (shown using arrows) obtained using the algorithm.\n"
      ],
      "metadata": {
        "id": "PacHcCfoBdij"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "upvFlf7Mx68D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(e) Hyper-parameters (5 Points)\n",
        "Write about the hyper-parameters you finally used for the algorithm and describe how did you arrive at\n",
        "these set of hyper-params. (5 Points)\n"
      ],
      "metadata": {
        "id": "I7CJ5c84Bdf1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "J6By9p2rIs_7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(f) Observations (5 Points)\n",
        "Write about your observations from the plots above."
      ],
      "metadata": {
        "id": "EmOhHhXVBddY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "ebaVM5qNIt8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem 7: SARSA(λ) Accumulating**\n",
        "(40+20+20+5+5+5+5=100 points)\n",
        "\n",
        "Implement the SARSA(λ) algorithm with Accumulating Eligibility Traces for the Random Maze Environment\n",
        "(RME) described above. Use the function definition as given in Lecture slides.\n",
        "\n",
        "SARSA-Lambda(env, $\\gamma$, $\\alpha_0$, $\\epsilon_0$, $\\lambda$, noEpisodes, replaceTrace = False)\n",
        "(40 Points)"
      ],
      "metadata": {
        "id": "ggBak-9RA6uG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sarsa_lambda_control(environment, config=None):\n",
        "    \"\"\"\n",
        "    Implements the SARSA(λ) algorithm with eligibility traces for a discrete\n",
        "    Random Maze Environment (RME).\n",
        "\n",
        "    This function applies SARSA(λ), an on-policy Temporal Difference (TD) control\n",
        "    algorithm that extends SARSA by incorporating eligibility traces to speed up\n",
        "    learning. Eligibility traces allow credit assignment to multiple previously\n",
        "    visited state-action pairs. Both accumulating and replacing traces are\n",
        "    supported, as specified in the configuration.\n",
        "\n",
        "    Args:\n",
        "        environment (gym.Env): Environment with discrete observation and action spaces.\n",
        "        config (dict): Configuration dictionary containing:\n",
        "            - max_episodes (int): Number of training episodes.\n",
        "            - discount_factor (float): Discount factor γ.\n",
        "            - lambda (float): Eligibility trace decay parameter λ.\n",
        "            - trace (str): Type of eligibility trace ('accumulating' or 'replacing').\n",
        "            - step_size (dict): Learning rate (alpha) decay parameters.\n",
        "            - epsilon (dict): Epsilon-greedy exploration decay parameters.\n",
        "            - seed (int): Random seed for environment reset.\n",
        "\n",
        "    Returns:\n",
        "        Q (np.ndarray): Final learned action-value function of shape (S, A).\n",
        "        V_s (np.ndarray): State-value estimates derived from Q for each episode.\n",
        "        policy (np.ndarray): Final greedy policy derived from Q.\n",
        "        Qs (np.ndarray): History of Q-values across episodes.\n",
        "        td_target (float): Last computed TD target value.\n",
        "        policy_success (np.ndarray): Boolean success indicator per episode.\n",
        "        return_reward (np.ndarray): Total return obtained per episode.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    pass"
      ],
      "metadata": {
        "id": "sleXKqZl35q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(a) State-value (V) function (20 Points)\n",
        "Plot evolution of State-value (V) function with time. Basically, plot V-function vs Episodes for states 0,\n",
        "1, 2, 4, 6, 8, 9, 10, 11 in a single plot. Also plot the true V-function for each of the state in the same plot.\n"
      ],
      "metadata": {
        "id": "7vye5JBMA6n-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_episode_values()"
      ],
      "metadata": {
        "id": "P1u-L03a3_c3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(b) Action state value (Q) function (20 Points)\n",
        "Plot evolution of action state value (Q) function with time. Plot Q function vs Episodes for states 0, 1,\n",
        "2, 4, 6, 8, 9, 10, 11 in a single plot. Also plot the true Q-function for each of the state in the same plot.\n"
      ],
      "metadata": {
        "id": "ewvD3djGA6ln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_episode_q_values()"
      ],
      "metadata": {
        "id": "zkH7EQq04DKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(c) Instances and seeds (5 Points)\n",
        "Describe over how many instances of the environments did you average the results? Write about the seeds\n",
        "used for each instance. (5 Points)\n"
      ],
      "metadata": {
        "id": "7jmJdF0PA6i8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "3A9ZDqgz5FLI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(d) Environment diagram (5 Points)\n",
        "Draw the environment diagram with optimal policy (shown using arrows) obtained using the algorithm.\n"
      ],
      "metadata": {
        "id": "NUqb7aKmA6fe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "1mc2oY7q5IjU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(e) Hyper-parameters (5 Points)\n",
        "Write about the hyper-parameters you finally used for the algorithm and describe how did you arrive at\n",
        "these set of hyper-params.\n"
      ],
      "metadata": {
        "id": "_pLcQnHdA6Vn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "htK_7PttIvfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(f) Observations (5 Points)\n",
        "Write about your observations from the plots above."
      ],
      "metadata": {
        "id": "mi4Imav-BWUO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "0Y2PRTn-IwPR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem 8: Q($\\lambda$) Replacing**\n",
        "($40+20+20+5+5+5+5=100$ points)\n",
        "\n",
        "Implement the Q(λ) algorithm with Replacing Eligibility Traces for the Random Maze Environment (RME)\n",
        "described above. Use the function definition as given in Lecture slides.\n",
        "\n",
        "Q-Lambda(env, $\\gamma$, $\\alpha_0$, $\\epsilon_0$, $\\lambda$, noEpisodes, replaceTrace = True)\n",
        "(40 Points)"
      ],
      "metadata": {
        "id": "1I44zCnpAUwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def q_learning_lambda_control(environment, config=None):\n",
        "    \"\"\"\n",
        "    Implements the Q(λ) algorithm with eligibility traces for a discrete\n",
        "    Random Maze Environment (RME).\n",
        "\n",
        "    This function applies Q(λ), an off-policy Temporal Difference (TD) control\n",
        "    algorithm that extends Q-Learning by incorporating eligibility traces.\n",
        "    Replacing or accumulating eligibility traces can be used to propagate\n",
        "    temporal-difference errors across multiple previously visited state-action\n",
        "    pairs. Trace resetting is handled according to whether the behavior policy\n",
        "    follows the greedy action.\n",
        "\n",
        "    Args:\n",
        "        environment (gym.Env): Environment with discrete observation and action spaces.\n",
        "        config (dict): Configuration dictionary containing:\n",
        "            - max_episodes (int): Number of training episodes.\n",
        "            - discount_factor (float): Discount factor γ.\n",
        "            - lambda (float): Eligibility trace decay parameter λ.\n",
        "            - trace (str): Type of eligibility trace ('accumulating' or 'replacing').\n",
        "            - step_size (dict): Learning rate (alpha) decay parameters.\n",
        "            - epsilon (dict): Epsilon-greedy exploration decay parameters.\n",
        "            - seed (int): Random seed for environment reset.\n",
        "\n",
        "    Returns:\n",
        "        Q (np.ndarray): Final learned action-value function of shape (S, A).\n",
        "        V_s (np.ndarray): State-value estimates derived from Q for each episode.\n",
        "        policy (np.ndarray): Final greedy policy derived from Q.\n",
        "        Qs (np.ndarray): History of Q-values across episodes.\n",
        "        td_target (float): Last computed TD target value.\n",
        "        policy_success (np.ndarray): Boolean success indicator per episode.\n",
        "        return_reward (np.ndarray): Total return obtained per episode.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "-sI7W2ei5mUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(a) State-value (V) function (20 Points)\n",
        "Plot evolution of State-value (V) function with time. Basically, plot V-function vs Episodes for states 0,\n",
        "1, 2, 4, 6, 8, 9, 10, 11 in a single plot. Also plot the true V-function for each of the state in the same plot.\n"
      ],
      "metadata": {
        "id": "nlmq_Pj6AdUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_episode_values()"
      ],
      "metadata": {
        "id": "LW8EoBdF5zc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###(b) Action state value (Q) function (20 Points)\n",
        "Plot evolution of action state value (Q) function with time. Plot Q function vs Episodes for states 0, 1,\n",
        "2, 4, 6, 8, 9, 10, 11 in a single plot. Also plot the true Q-function for each of the state in the same plot.\n"
      ],
      "metadata": {
        "id": "VVRtgqQeAmvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_episode_q_values()"
      ],
      "metadata": {
        "id": "AxYjjxbS50YA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(c) Instances and seeds (5 Points)\n",
        "Describe over how many instances of the environments did you average the results? Write about the seeds\n",
        "used for each instance.\n"
      ],
      "metadata": {
        "id": "QOKOY0eAAp8i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "HgK3UjRT6ClA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(d) Environment diagram (5 Points)\n",
        "Draw the environment diagram with optimal policy (shown using arrows) obtained using the algorithm.\n"
      ],
      "metadata": {
        "id": "7Qzz6kqqAs5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "uCJib09n6Equ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(e) Hyper-parameters (5 Points)\n",
        "Write about the hyper-parameters you finally used for the algorithm and describe how did you arrive at\n",
        "these set of hyper-params.\n"
      ],
      "metadata": {
        "id": "2n_5Wqb9AwlX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "OnZt78HAIx1X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (f) Observations (5 Points)\n",
        "Write about your observations from the plots above."
      ],
      "metadata": {
        "id": "-CzFZ1t4A00-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "w8kgl_qyIywI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem 9: Q(λ) Accumulating**\n",
        "\n",
        "($40+20+20+5+5+5+5=100$ points)\n",
        "\n",
        "Implement the Q(λ) algorithm with Accumulating Eligibility Traces for the Random Maze Environment (RME)\n",
        "described above. Use the function definition as given in Lecture slides.\n",
        "\n",
        "Q-Lambda(env, $\\gamma$, $\\alpha_0$, $\\epsilon_0$, $\\lambda$, noEpisodes, replaceTrace = False)\n",
        "(40 Points)"
      ],
      "metadata": {
        "id": "qAUraoBY_tHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def q_learning_lambda_control(environment, config=None):\n",
        "    \"\"\"\n",
        "    Implements the Q(λ) algorithm with eligibility traces for a discrete\n",
        "    Random Maze Environment (RME).\n",
        "\n",
        "    This function applies Q(λ), an off-policy Temporal Difference (TD) control\n",
        "    algorithm that extends Q-Learning by incorporating eligibility traces.\n",
        "    Replacing or accumulating eligibility traces can be used to propagate\n",
        "    temporal-difference errors across multiple previously visited state-action\n",
        "    pairs. Trace resetting is handled according to whether the behavior policy\n",
        "    follows the greedy action.\n",
        "\n",
        "    Args:\n",
        "        environment (gym.Env): Environment with discrete observation and action spaces.\n",
        "        config (dict): Configuration dictionary containing:\n",
        "            - max_episodes (int): Number of training episodes.\n",
        "            - discount_factor (float): Discount factor γ.\n",
        "            - lambda (float): Eligibility trace decay parameter λ.\n",
        "            - trace (str): Type of eligibility trace ('accumulating' or 'replacing').\n",
        "            - step_size (dict): Learning rate (alpha) decay parameters.\n",
        "            - epsilon (dict): Epsilon-greedy exploration decay parameters.\n",
        "            - seed (int): Random seed for environment reset.\n",
        "\n",
        "    Returns:\n",
        "        Q (np.ndarray): Final learned action-value function of shape (S, A).\n",
        "        V_s (np.ndarray): State-value estimates derived from Q for each episode.\n",
        "        policy (np.ndarray): Final greedy policy derived from Q.\n",
        "        Qs (np.ndarray): History of Q-values across episodes.\n",
        "        td_target (float): Last computed TD target value.\n",
        "        policy_success (np.ndarray): Boolean success indicator per episode.\n",
        "        return_reward (np.ndarray): Total return obtained per episode.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    pass"
      ],
      "metadata": {
        "id": "mWNtJjiU6bOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(a) State-value (V) function (20 Points)\n",
        "Plot evolution of State-value (V) function with time. Basically, plot V-function vs Episodes for states 0,\n",
        "1, 2, 4, 6, 8, 9, 10, 11 in a single plot. Also plot the true V-function for each of the state in the same plot.\n"
      ],
      "metadata": {
        "id": "pYntmxhf_tBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_episode_values()"
      ],
      "metadata": {
        "id": "EHIb3ybW6rc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(b) Action state value (Q) function (20 Points)\n",
        "Plot evolution of action state value (Q) function with time. Plot Q function vs Episodes for states 0, 1,\n",
        "2, 4, 6, 8, 9, 10, 11 in a single plot. Also plot the true Q-function for each of the state in the same plot.\n"
      ],
      "metadata": {
        "id": "oXQhf_Kf_s-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_episode_q_values()"
      ],
      "metadata": {
        "id": "XrUD4un86sk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(c) Instances and seeds (5 Points)\n",
        "Describe over how many instances of the environments did you average the results? Write about the seeds\n",
        "used for each instance.\n"
      ],
      "metadata": {
        "id": "TCgf3IpH_s8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "pyA_CsPX6ydy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(d)Environment diagram (5 Points)\n",
        "Draw the environment diagram with optimal policy (shown using arrows) obtained using the algorithm.\n"
      ],
      "metadata": {
        "id": "FVK8BITZ_s5z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "p0rPK2BJ6xNt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(e) Hyper-parameters (5 Points)\n",
        "Write about the hyper-parameters you finally used for the algorithm and describe how did you arrive at\n",
        "these set of hyper-params.\n",
        "\n"
      ],
      "metadata": {
        "id": "fL9jbNZF_s3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "rnlc3cSwI0l1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(f) Observations (5 Points)\n",
        "Write about your observations from the plots above."
      ],
      "metadata": {
        "id": "62gjt7Rl_stE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "Pnj_6yoLI4wF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem 10: Dyna-Q**\n",
        "\n",
        "($40+20+20+5+5+5+5=100$ points)\n",
        "\n",
        "Implement the Dyna-Q algorithm for the Random Maze Environment (RME) described above. Use the function\n",
        "definition as given in Lecture slides.\n",
        "\n",
        "Dyna-Q(env, $\\gamma$, $\\alpha_0$, $\\epsilon_0$, noEpisodes, noPlanning)\n",
        "(40 Points)"
      ],
      "metadata": {
        "id": "H80eXV3Y--hE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dyna_q_control(environment, config=None):\n",
        "    \"\"\"\n",
        "    Implements the Dyna-Q algorithm for a discrete Random Maze Environment (RME).\n",
        "\n",
        "    This function applies Dyna-Q, a model-based reinforcement learning algorithm\n",
        "    that integrates direct learning from real environment interactions with\n",
        "    planning updates using a learned model of the environment. The agent learns\n",
        "    an action-value function Q(s, a) while simultaneously estimating the\n",
        "    transition dynamics and reward model, which are used for simulated planning\n",
        "    updates.\n",
        "\n",
        "    Args:\n",
        "        environment (gym.Env): Environment with discrete observation and action spaces.\n",
        "        config (dict): Configuration dictionary containing:\n",
        "            - max_episodes (int): Number of training episodes.\n",
        "            - discount_factor (float): Discount factor γ.\n",
        "            - no_planning_steps (int): Number of planning updates per real step.\n",
        "            - step_size (dict): Learning rate (alpha) decay parameters.\n",
        "            - epsilon (dict): Epsilon-greedy exploration decay parameters.\n",
        "            - seed (int): Random seed for environment reset.\n",
        "\n",
        "    Returns:\n",
        "        Q (np.ndarray): Final learned action-value function of shape (S, A).\n",
        "        V_s (np.ndarray): State-value estimates derived from Q for each episode.\n",
        "        policy (np.ndarray): Final greedy policy derived from Q.\n",
        "        Qs (np.ndarray): History of Q-values across episodes.\n",
        "        None: Placeholder (no TD target history stored).\n",
        "        policy_success (np.ndarray): Boolean success indicator per episode.\n",
        "        return_reward (np.ndarray): Total return obtained per episode.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "5-jQBR6t7F_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(a)State-value (V) function (20 Points)\n",
        "Plot evolution of State-value (V) function with time. Basically, plot V-function vs Episodes for states 0,\n",
        "1, 2, 4, 6, 8, 9, 10, 11 in a single plot. Also plot the true V-function for each of the state in the same plot."
      ],
      "metadata": {
        "id": "sJ2lrRhM_TY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_episode_values()"
      ],
      "metadata": {
        "id": "VntdiCRB7Jy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(b) Action state value (Q) function (20 Points)\n",
        "Plot evolution of action state value (Q) function with time. Plot Q function vs Episodes for states 0, 1,\n",
        "2, 4, 6, 8, 9, 10, 11 in a single plot. Also plot the true Q-function for each of the state in the same plot.\n"
      ],
      "metadata": {
        "id": "hngB3uf-_Yqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_episode_q_values()"
      ],
      "metadata": {
        "id": "8AIiOGSQ7O7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(c) Instances and seeds (5 Points)\n",
        "Describe over how many instances of the environments did you average the results? Write about the seeds\n",
        "used for each instance."
      ],
      "metadata": {
        "id": "Nwt1x_gv_cRm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "hgbuf6Jw7Qr4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(d) Environment diagram (5 Points)\n",
        "Draw the environment diagram with optimal policy (shown using arrows) obtained using the algorithm."
      ],
      "metadata": {
        "id": "FvSey9SP_geW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "dh3cbn3c7Um1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(e) Hyper-parameters (5 Points)\n",
        "Write about the hyper-parameters you finally used for the algorithm and describe how did you arrive at\n",
        "these set of hyper-params.\n"
      ],
      "metadata": {
        "id": "8tfVNZB6_j3Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "DJ-oAojII9l7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(f) Observations (5 Points)\n",
        "Write about your observations from the plots above.\n"
      ],
      "metadata": {
        "id": "a9G09qoD_nFh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "cPCbgyM3I_bK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem 11: Trajectory Learning**\n",
        "\n",
        "($40+20+20+5+5+5+5=100$ points)\n",
        "\n",
        "Implement the Trajectory Learning algorithm for the Random Maze Environment (RME) described above.\n",
        "Use the function definition as given in Lecture slides.\n",
        "\n",
        "TrajectorySampling(env, γ, α0, ϵ0, noEpisodes, maxTrajectory)\n",
        "(40 Points)"
      ],
      "metadata": {
        "id": "L2qJiA-6-T9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trajectory_sampling_control(environment, config=None):\n",
        "    \"\"\"\n",
        "    Trajectory Sampling (Trajectory Learning) Control Algorithm\n",
        "\n",
        "    Algorithm (from lecture slides):\n",
        "        TrajectorySampling(env, γ, α0, ε0, noEpisodes, maxTrajectory)\n",
        "\n",
        "    Goal:\n",
        "        Learn an optimal action-value function Q(s, a) by combining:\n",
        "            - Direct interaction with the environment\n",
        "            - A learned model of the environment\n",
        "            - Simulated trajectories sampled from the learned model\n",
        "\n",
        "    The algorithm alternates between:\n",
        "        1. Real experience updates (Q-learning)\n",
        "        2. Planning updates via trajectory sampling\n",
        "\n",
        "    Inputs:\n",
        "        environment : Random Maze Environment (RME)\n",
        "        config      : Dictionary containing γ, α0, ε0, number of episodes,\n",
        "                      maximum trajectory length, and sampling probability\n",
        "    \"\"\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WM2NQOcy8xJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(a) State-value (V) function (20 Points)\n",
        "Plot evolution of State-value (V) function with time. Basically, plot V-function vs Episodes for states 0,\n",
        "1, 2, 4, 6, 8, 9, 10, 11 in a single plot. Also plot the true V-function for each of the state in the same plot."
      ],
      "metadata": {
        "id": "RFh7FKwW-Tot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_episode_values()"
      ],
      "metadata": {
        "id": "XcQ_a6VV86Ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(b) Action state value (Q) function (20 Points)\n",
        "Plot evolution of action state value (Q) function with time. Plot Q function vs Episodes for states 0, 1,\n",
        "2, 4, 6, 8, 9, 10, 11 in a single plot. Also plot the true Q-function for each of the state in the same plot."
      ],
      "metadata": {
        "id": "bYW4H8Ce-Tef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_episode_q_values()"
      ],
      "metadata": {
        "id": "bcQI6lD987cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(c) Instances and seeds (5 Points)\n",
        "Describe over how many instances of the environments did you average the results? Write about the seeds\n",
        "used for each instance.\n"
      ],
      "metadata": {
        "id": "Ue0nw0rW-TUu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "nxtiqHII9BOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(d)Environment diagram (5 Points)\n",
        "Draw the environment diagram with optimal policy (shown using arrows) obtained using the algorithm.\n"
      ],
      "metadata": {
        "id": "0s7-U9wW-TK-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "jehpz3rV9AHt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(e) Hyper-parameters (5 Points)\n",
        "Write about the hyper-parameters you finally used for the algorithm and describe how did you arrive at\n",
        "these set of hyper-params."
      ],
      "metadata": {
        "id": "lyiZNzRr-TBh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "VndyiZD5JA5B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(f) Observations (5 Points)\n",
        "Write about your observations from the plots above."
      ],
      "metadata": {
        "id": "LZbH7rnQ-S3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "V0d-0FvjJF5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem 12: Comparing Control Algorithms**\n",
        "\n",
        "($5+5+5+5+5+5=25$ points)\n",
        "\n",
        "For SARSA(λ) Replacing, SARSA(λ) Accumulating, Q(λ) Replacing, Q(λ) Accumulating, Dyna-Q, Trajectory Learning implemented above, do the following:\n",
        "\n",
        "###(a) Policy Success Rate (in %) vs Episodes (5 Points)\n",
        "For each of the algorithm, in a single plot, plot the evolution of Policy Success Rate (in %) vs Episodes.(5 Points)"
      ],
      "metadata": {
        "id": "9QiBJn_C9IKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_policy_success(policy_success, title, image_name, config=None):\n",
        "    \"\"\"\n",
        "    Plot Policy Success Rate (%) vs Episodes for multiple algorithms.\n",
        "\n",
        "    Task:\n",
        "        For each algorithm, plot the evolution of policy success rate\n",
        "        (in percentage) as training progresses.\n",
        "\n",
        "    Definition:\n",
        "        Policy Success Rate =\n",
        "            (Number of times the agent reaches the goal state) /\n",
        "            (Total number of evaluation episodes)\n",
        "\n",
        "    Inputs:\n",
        "        policy_success : Array containing policy success outcomes across\n",
        "                         episodes and multiple runs for each algorithm\n",
        "        title          : Title of the plot\n",
        "        image_name     : Filename to save the plot\n",
        "        config         : Configuration dictionary containing max_episodes\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "BN0Uxey_9MFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(b) Policy Success Rate (in %) plot (5 Points)\n",
        "What are your observations from the Policy Success Rate (in %) plot.(5 Points)"
      ],
      "metadata": {
        "id": "VsecUwrr9IZl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "fjA_LWAY9NMH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(c) Estimated Expected Return (5 Points)\n",
        "For each of the algorithm (in a single plot), plot the Estimated Expected Return (from the start state) vs\n",
        "Episodes."
      ],
      "metadata": {
        "id": "kf2olQPE9IdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_expected_return(return_reward, title, image_name, config=None):\n",
        "    \"\"\"\n",
        "    Plot Estimated Expected Return (from the start state) vs Episodes\n",
        "    for multiple reinforcement learning algorithms.\n",
        "\n",
        "    Task:\n",
        "        For each algorithm, plot how the estimated expected return\n",
        "        from the start state evolves as training progresses.\n",
        "\n",
        "    Definition:\n",
        "        Estimated Expected Return =\n",
        "            Average cumulative reward obtained by following the\n",
        "            current policy from the start state.\n",
        "\n",
        "    Inputs:\n",
        "        return_reward : Array containing estimated returns across\n",
        "                        episodes and multiple runs for each algorithm\n",
        "        title         : Title of the plot\n",
        "        image_name    : Filename to save the plot\n",
        "        config        : Configuration dictionary containing max_episodes\n",
        "    \"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "1hqeWBH79PhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(d) Observations (5 Points)\n",
        "What are your observations for the Estimated Expected Return plot?"
      ],
      "metadata": {
        "id": "fL1Tu1As9tvj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "B1fRUg0xJQRX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(e) State-value Function Estimation Error (5 Points)\n",
        "For each of the algorithm (in a single plot), plot the State-value Function Estimation Error vs Episodes.\n",
        "State-value Function Estimation Error is defined as Mean Absolute Error across all V-function estimates\n",
        "(across all states) from the respective optimal value."
      ],
      "metadata": {
        "id": "3-aLXdIH9w7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_state_value_error(state_values, title, image_name, config=None):\n",
        "    \"\"\"\n",
        "    Plot State-value Function Estimation Error vs Episodes\n",
        "    for multiple reinforcement learning algorithms.\n",
        "\n",
        "    Task:\n",
        "        For each algorithm, plot how the error in the estimated\n",
        "        state-value function decreases over training episodes.\n",
        "\n",
        "    Definition:\n",
        "        State-value Function Estimation Error =\n",
        "            Mean Absolute Error (MAE) between the estimated V(s)\n",
        "            and the true optimal V*(s), averaged across all\n",
        "            non-terminal states.\n",
        "\n",
        "    Inputs:\n",
        "        state_values : Array containing estimated V-values for all\n",
        "                       states, episodes, and algorithms\n",
        "        title        : Title of the plot\n",
        "        image_name   : Filename to save the plot\n",
        "        config       : Configuration dictionary containing max_episodes\n",
        "    \"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "k6xbLi_v9V7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(f) Observations (5 Points)\n",
        "What are your observations for the State-value Function Estimation Error plot?"
      ],
      "metadata": {
        "id": "RZWoaKEW9zTa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Write your answer here]"
      ],
      "metadata": {
        "id": "6z20zSIuJLd8"
      }
    }
  ]
}